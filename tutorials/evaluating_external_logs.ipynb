{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating external Logs on Humanloop\n",
    "\n",
    "This script demonstrates how to run an Evaluation on Humanloop using external Logs.\n",
    "This is useful if you have existing Logs in an external system and you want to evaluate them on Humanloop.\n",
    "\n",
    "The script demonstrates how to:\n",
    "- Upload Logs to Humanloop\n",
    "- Create an Evaluation on Humanloop\n",
    "- Add Logs to a Run\n",
    "- Alternatively, upload Logs directly to a Run\n",
    "\n",
    "\n",
    "In this example, we will upload a JSON file containing chat messages between users\n",
    "and customer support agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(os.getcwd()).parent / \"assets\" / \"conversations-a.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(data[:2], width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load .env file that contains API keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from humanloop import Humanloop\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"), base_url=os.getenv(\"HUMANLOOP_BASE_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Logs to Humanloop\n",
    "\n",
    "One way to start using your Logs is to upload them to Humanloop. Here, we'll upload the Logs to a **Flow** on Humanloop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "log_ids = []\n",
    "for messages in tqdm(data):\n",
    "    log = humanloop.flows.log(\n",
    "        path=\"External logs demo/Travel planner\",\n",
    "        flow={\"attributes\": {\"agent-version\": \"1.0.0\"}},  # Optionally add attributes to identify this version of the support agent.\n",
    "        messages=messages,\n",
    "    )\n",
    "    log_ids.append(log.id)\n",
    "\n",
    "version_id = log.version_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Evaluation on Humanloop\n",
    "\n",
    "Next, we'll create an Evaluation on Humanloop. This will allow us to evaluate the Logs we uploaded.\n",
    "An Evaluation will have a set of Runs, each of which will have a set of Logs, allowing us to compare the performance across different Runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation\n",
    "evaluation = humanloop.evaluations.create(\n",
    "    name=\"Past records\",\n",
    "    # NB: you can use `path`or `id` for references on Humanloop\n",
    "    file={\"path\": \"External logs demo/Travel planner\"},\n",
    "    evaluators=[\n",
    "        {\"path\": \"Example Evaluators/Human/rating\"},\n",
    "    ],\n",
    ")\n",
    "print(f\"Created Evaluation: {evaluation.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Run for this set of Logs\n",
    "run = humanloop.evaluations.create_run(\n",
    "    id=evaluation.id,\n",
    "    version={'version_id': version_id},  # Associate this Run to the Flow version created above.\n",
    ")\n",
    "print(f\"Created Run: {run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Logs to the Run\n",
    "humanloop.evaluations.add_logs_to_run(\n",
    "  id=evaluation.id,\n",
    "    run_id=run.id,\n",
    "    log_ids=log_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created an Evaluation on Humanloop and added Logs to it. You can now view the Evaluation on the Humanloop UI.\n",
    "\n",
    "![Evaluation on Humanloop](../assets/images/external_logs_evaluations.png)\n",
    "\n",
    "And you can view the Logs in the **Logs** tab.\n",
    "\n",
    "![Log in Review tab](../assets/images/external_logs_logs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Logs to a Run\n",
    "\n",
    "As an alternative pattern, you can upload Logs directly to a Run. This is appropriate if you have an existing Evaluation on Humanloop that you want to create a new Run for.\n",
    "\n",
    "In short, to do this, you'll need to pass `run_id` into your `log(...)` calls.\n",
    "\n",
    "In this example, we will use the Evaluation we created earlier and add a new set of Logs to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data\n",
    "with open(Path(os.getcwd()).parent / \"assets\" / \"conversations-b.json\") as f:\n",
    "    new_data = json.load(f)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(new_data[:2], width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previously-created Evaluation\n",
    "evaluation_id = evaluation.id\n",
    "evaluation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Run in the same Evaluation\n",
    "new_run = humanloop.evaluations.create_run(\n",
    "    id=evaluation.id,\n",
    ")\n",
    "print(f\"Created new Run: {new_run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to the Run\n",
    "for messages in tqdm(new_data):\n",
    "    log = humanloop.flows.log(\n",
    "        path=\"External logs demo/Travel planner\",\n",
    "        flow={\"attributes\": {\"agent-version\": \"2.0.0\"}},\n",
    "        messages=messages,\n",
    "        # Pass `run_id` to associate the Log with the Run.\n",
    "        run_id=new_run.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now added a second Run to the Evaluation and populated it with Logs. You can now view the Run on the Humanloop UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The above examples demonstrates how you can quickly populate an Evaluation Run with your logs. This allows you to utilise the Evaluation and Evaluator functionalities to perform workflows such as using [Code Evaluators](https://humanloop.com/docs/v5/guides/evals/code-based-evaluator) to calculate metrics, or using [Human Evaluators](https://humanloop.com/docs/v5/guides/evals/human-evaluators) to set up your Logs to be reviewed by your subject-matter experts.\n",
    "\n",
    "Refer to our [documentation](https://humanloop.com/docs/v5/guides/evals) for more information on how to set up custom Evaluators and extend the Evaluation for your use-case.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
