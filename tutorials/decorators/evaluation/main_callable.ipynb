{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Files declared in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlights how to combine Humanloop decorated files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from humanloop import Humanloop\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "from levenshtein import compare_log_and_target\n",
    "from exact_match import exact_match\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiating the Humanloop client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanloop = Humanloop(api_key=os.getenv(\"HL_API_KEY\"), base_url=\"http://0.0.0.0:80/v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiating the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = chromadb.chromadb.Client()\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# init collection into which we will add documents\n",
    "knowledge_base = pd.read_parquet(\"../../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(5, random_state=42)\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_df = pd.read_json(\"../../../assets/datapoints.jsonl\", lines=True)\n",
    "datapoints = [row.to_dict() for _, row in datapoints_df.iterrows()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Declare Humanloop Files via code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "- {option_A}\n",
    "- {option_B}\n",
    "- {option_C}\n",
    "- {option_D}\n",
    "- {option_E}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{retrieved_data}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "def ask_model(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    ") -> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(question)\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"option_A\": option_A,\n",
    "        \"option_B\": option_B,\n",
    "        \"option_C\": option_C,\n",
    "        \"option_D\": option_D,\n",
    "        \"option_E\": option_E,\n",
    "        \"retrieved_data\": retrieved_data,\n",
    "    }\n",
    "\n",
    "    # Populate the Prompt template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": TEMPLATE.format(**inputs),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def entrypoint(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    "):\n",
    "    return ask_model(\n",
    "        question=question,\n",
    "        option_A=option_A,\n",
    "        option_B=option_B,\n",
    "        option_C=option_C,\n",
    "        option_D=option_D,\n",
    "        option_E=option_E,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[96mEvaluating your flow function corresponding to `Evaluations SDK Demo/MedQA Answer Flow 7` on Humanloop\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\u001b[96mNavigate to your Evaluation:\u001b[0m\n",
      "http://localhost:3000/project/fl_QC8vytIlRDSiBZn3aQ3je/evaluations/evr_UudOImmhU7y2WOd2wDdZT/stats\n",
      "\n",
      "\u001b[96mFlow Version ID: flv_wrXlOmVdz7ICipmBmfwrn\u001b[0m\n",
      "\u001b[96mRun ID: rn_MtSSKpEn5nnl1anD1cr7A\u001b[0m\n",
      "\u001b[96m\n",
      "Running 'MedQA Answer Flow 7' over the Dataset 'Dataset' using 8 workers\u001b[0m \n",
      "[########################################] 20/20 (100.00%) | DONE1ss\n",
      "\n",
      "\u001b[96m‚è≥ Evaluation Progress\u001b[0m\n",
      "Total Logs: 19\n",
      "Total Judgments: 34\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m‚è≥ Evaluation Progress\u001b[0m\n",
      "Total Logs: 23\n",
      "Total Judgments: 42\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96müìä Evaluation Results for Evaluations SDK Demo/MedQA Answer Flow 7 \u001b[0m\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "|                                  |       Control       |        Latest       |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "|                           Run ID |        FyenY        |        MtSSK        |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "|                       Version ID |         None        |         None        |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "|                            Added | 2024-11-11 14:13:21 | 2024-11-11 14:21:16 |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "|                       Evaluators |                     |                     |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "| Evaluations SDK Demo/Levenshtein |         N/A         |         5.45        |\n",
      "| Evaluations SDK Demo/Exact Match |         N/A         |         0.8         |\n",
      "+----------------------------------+---------------------+---------------------+\n",
      "\n",
      "\n",
      "\u001b[96mChange of [-45.55] for Evaluator Evaluations SDK Demo/Levenshtein\u001b[0m\n",
      "\u001b[96mChange of [0.8] for Evaluator Evaluations SDK Demo/Exact Match\u001b[0m\n",
      "\n",
      "\u001b[96mView your Evaluation:\u001b[0m\n",
      "http://localhost:3000/project/fl_QC8vytIlRDSiBZn3aQ3je/evaluations/evr_UudOImmhU7y2WOd2wDdZT/stats\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EvaluatorCheck(path='Evaluations SDK Demo/Levenshtein', score=5.45, delta=-45.55, threshold=None, threshold_check=None, evaluation_id='evr_UudOImmhU7y2WOd2wDdZT'),\n",
       " EvaluatorCheck(path='Evaluations SDK Demo/Exact Match', score=0.8, delta=0.8, threshold=None, threshold_check=None, evaluation_id='evr_UudOImmhU7y2WOd2wDdZT')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanloop.evaluations.run(\n",
    "    file={\n",
    "        \"path\": \"Evaluations SDK Demo/MedQA Answer Flow 7\",\n",
    "        \"callable\": entrypoint,\n",
    "        \"version\": {\n",
    "            \"attributes\": {\n",
    "                \"prompt\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"environment\": \"evaluation\",\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"flow\",\n",
    "    },\n",
    "    name=\"MedQA Evaluation Decorators Debug\",\n",
    "    dataset={\n",
    "        \"datapoints\": datapoints,\n",
    "        \"path\": \"Evaluations SDK Demo/Dataset\",\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"path\": \"Evaluations SDK Demo/Levenshtein\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"number\",\n",
    "            \"callable\": compare_log_and_target,\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"Evaluations SDK Demo/Exact Match\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"boolean\",\n",
    "            \"callable\": exact_match,\n",
    "        },\n",
    "    ],\n",
    "    workers=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanloop-cookbook-XVtM0WJo-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
