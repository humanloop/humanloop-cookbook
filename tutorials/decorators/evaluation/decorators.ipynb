{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Files declared in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlights how to combine Humanloop decorated files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from exact_match import exact_match\n",
    "from humanloop import Humanloop\n",
    "from levenshtein import compare_log_and_target\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrei QA/SDK/Decorators Evaluation\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"SDK/Decorators Evaluation\"\n",
    "# DIRECTORY_PREFIX should lack a trailing slash\n",
    "if DIRECTORY_PREFIX := os.getenv(\"DIRECTORY_PREFIX\"):\n",
    "    DIRECTORY = f\"{DIRECTORY_PREFIX}/{DIRECTORY}\"\n",
    "print(DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiating the Humanloop client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiating the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = chromadb.chromadb.Client()\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# init collection into which we will add documents\n",
    "knowledge_base = pd.read_parquet(\"../../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(5, random_state=42)\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_df = pd.read_json(\"../../../assets/datapoints.jsonl\", lines=True)\n",
    "datapoints = [row.to_dict() for _, row in datapoints_df.iterrows()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Declare Humanloop Files via code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "- {option_A}\n",
    "- {option_B}\n",
    "- {option_C}\n",
    "- {option_D}\n",
    "- {option_E}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{retrieved_data}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@humanloop.tool(\n",
    "    path=f\"{DIRECTORY}/Retrieval Tool\",\n",
    ")\n",
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "@humanloop.prompt(\n",
    "    path=f\"{DIRECTORY}/MedQA Answer\",\n",
    "    template=TEMPLATE,\n",
    "    tools=[retrieval_tool.json_schema],\n",
    ")\n",
    "def ask_model(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    ") -> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(question)\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"option_A\": option_A,\n",
    "        \"option_B\": option_B,\n",
    "        \"option_C\": option_C,\n",
    "        \"option_D\": option_D,\n",
    "        \"option_E\": option_E,\n",
    "        \"retrieved_data\": retrieved_data,\n",
    "    }\n",
    "\n",
    "    # Populate the Prompt template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": TEMPLATE.format(**inputs),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "@humanloop.flow(\n",
    "    path=f\"{DIRECTORY}/MedQA Answer Flow\",\n",
    "    attributes={\n",
    "        \"prompt\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"environment\": \"evaluation\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "def entrypoint(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    "):\n",
    "    return ask_model(\n",
    "        question=question,\n",
    "        option_A=option_A,\n",
    "        option_B=option_B,\n",
    "        option_C=option_C,\n",
    "        option_D=option_D,\n",
    "        option_E=option_E,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[96mEvaluating your flow function corresponding to `Andrei QA/SDK/Decorators Evaluation/MedQA Answer Flow` on Humanloop\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\u001b[96mNavigate to your Evaluation:\u001b[0m\n",
      "https://app.humanloop.com/project/fl_TTxo2YKusqhfMHfQC9sGq/evaluations/evr_vv8Av5FQsHgOeXRtv0DHW/stats\n",
      "\n",
      "\u001b[96mFlow Version ID: flv_K5LsdlvCdscZh29TMFcDd\u001b[0m\n",
      "\u001b[96mRun ID: rn_mJ0pbOjJTGWby0zzjpFgP\u001b[0m\n",
      "\u001b[96m\n",
      "Running 'MedQA Answer Flow' over the Dataset 'Dataset' using 8 workers\u001b[0m \n",
      "\n",
      "\u001b[96m‚è≥ Evaluation Progress\u001b[0m\n",
      "Total Logs: 18\n",
      "Total Judgments: 36\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m‚è≥ Evaluation Progress\u001b[0m\n",
      "Total Logs: 20\n",
      "Total Judgments: 38\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m‚è≥ Evaluation Progress\u001b[0m\n",
      "Total Logs: 20\n",
      "Total Judgments: 40\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96müìä Evaluation Results for Andrei QA/SDK/Decorators Evaluation/MedQA Answer Flow \u001b[0m\n",
      "+-------------------------------------------------+---------------------+\n",
      "|                                                 |        Latest       |\n",
      "+-------------------------------------------------+---------------------+\n",
      "|                                          Run ID |        mJ0pb        |\n",
      "+-------------------------------------------------+---------------------+\n",
      "|                                      Version ID |         None        |\n",
      "+-------------------------------------------------+---------------------+\n",
      "|                                           Added | 2024-11-15 12:54:00 |\n",
      "+-------------------------------------------------+---------------------+\n",
      "|                                      Evaluators |                     |\n",
      "+-------------------------------------------------+---------------------+\n",
      "| Andrei QA/SDK/Decorators Evaluation/Levenshtein |         4.25        |\n",
      "| Andrei QA/SDK/Decorators Evaluation/Exact Match |         0.85        |\n",
      "+-------------------------------------------------+---------------------+\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96mView your Evaluation:\u001b[0m\n",
      "https://app.humanloop.com/project/fl_TTxo2YKusqhfMHfQC9sGq/evaluations/evr_vv8Av5FQsHgOeXRtv0DHW/stats\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################] 20/20 (100.00%) | DONE1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanloop.evaluations.run(\n",
    "    file={\n",
    "        \"path\": f\"{DIRECTORY}/MedQA Answer Flow\",\n",
    "        \"callable\": entrypoint,\n",
    "        \"type\": \"flow\",\n",
    "    },\n",
    "    name=\"MedQA Evaluation Decorators\",\n",
    "    dataset={\n",
    "        \"datapoints\": datapoints,\n",
    "        \"path\": f\"{DIRECTORY}/Dataset\",\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"path\": f\"{DIRECTORY}/Levenshtein\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"number\",\n",
    "            \"callable\": compare_log_and_target,\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{DIRECTORY}/Exact Match\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"boolean\",\n",
    "            \"callable\": exact_match,\n",
    "        },\n",
    "    ],\n",
    "    workers=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanloop-cookbook-XVtM0WJo-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
