{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Files declared in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlights how to combine Humanloop decorated files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from humanloop import Humanloop\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "from levenshtein import compare_log_and_target\n",
    "from exact_match import exact_match\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiating the Humanloop client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanloop = Humanloop(api_key=os.getenv(\"HL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiating the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = chromadb.chromadb.Client()\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# init collection into which we will add documents\n",
    "knowledge_base = pd.read_parquet(\"../../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(5, random_state=42)\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_df = pd.read_json(\"../../../assets/datapoints.jsonl\", lines=True)\n",
    "datapoints = [row.to_dict() for _, row in datapoints_df.iterrows()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Declare Humanloop Files via code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "- {option_A}\n",
    "- {option_B}\n",
    "- {option_C}\n",
    "- {option_D}\n",
    "- {option_E}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{retrieved_data}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "def ask_model(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    ") -> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(question)\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"option_A\": option_A,\n",
    "        \"option_B\": option_B,\n",
    "        \"option_C\": option_C,\n",
    "        \"option_D\": option_D,\n",
    "        \"option_E\": option_E,\n",
    "        \"retrieved_data\": retrieved_data,\n",
    "    }\n",
    "\n",
    "    # Populate the Prompt template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": TEMPLATE.format(**inputs),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def entrypoint(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    "):\n",
    "    output = ask_model(\n",
    "        question=question,\n",
    "        option_A=option_A,\n",
    "        option_B=option_B,\n",
    "        option_C=option_C,\n",
    "        option_D=option_D,\n",
    "        option_E=option_E,\n",
    "    )\n",
    "    humanloop.flows.log(\n",
    "        path=\"Evaluations SDK Demo/MedQA Answer Flow 8\",\n",
    "        flow={\n",
    "            \"attributes\": {\n",
    "                \"prompt\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"environment\": \"evaluation\",\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        inputs={\n",
    "            \"question\": question,\n",
    "            \"option_A\": option_A,\n",
    "            \"option_B\": option_B,\n",
    "            \"option_C\": option_C,\n",
    "            \"option_D\": option_D,\n",
    "            \"option_E\": option_E,\n",
    "        },\n",
    "        output=json.dumps({\"answer\": output}),\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[96mEvaluating your flow function corresponding to `Evaluations SDK Demo/MedQA Answer Flow 8` on Humanloop\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\u001b[96mNavigate to your Evaluation:\u001b[0m\n",
      "http://localhost:3000/project/fl_iiAjv1p4Kt7BUODEpsIro/evaluations/evr_Fe6hBqvelFS6mJCzmIqlw/stats\n",
      "\n",
      "\u001b[96mFlow Version ID: flv_jehMg9qF0zyxvLgE2bweR\u001b[0m\n",
      "\u001b[96mRun ID: rn_rEKHLYePrD4iTYO3ZjT5i\u001b[0m\n",
      "\u001b[96m\n",
      "Running 'MedQA Answer Flow 8' over the Dataset 'Dataset' using 8 workers\u001b[0m \n",
      "[##--------------------------------------] 1/20 (5.00%) | ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####------------------------------------] 2/20 (10.00%) | ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######----------------------------------] 3/20 (15.00%) | ETA: 13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########--------------------------------] 4/20 (20.00%) | ETA: 13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##########------------------------------] 5/20 (25.00%) | ETA: 15s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############----------------------------] 6/20 (30.00%) | ETA: 14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############--------------------------] 7/20 (35.00%) | ETA: 15s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[################------------------------] 8/20 (40.00%) | ETA: 13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################----------------------] 9/20 (45.00%) | ETA: 14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####################--------------------] 10/20 (50.00%) | ETA: 12s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################------------------] 11/20 (55.00%) | ETA: 15s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################----------------] 12/20 (60.00%) | ETA: 13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##########################--------------] 13/20 (65.00%) | ETA: 11s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[############################------------] 14/20 (70.00%) | ETA: 10s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##############################----------] 15/20 (75.00%) | ETA: 7s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[################################--------] 16/20 (80.00%) | ETA: 6s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################################------] 17/20 (85.00%) | ETA: 5s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[####################################----] 18/20 (90.00%) | ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################################--] 19/20 (95.00%) | ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################] 20/20 (100.00%) | DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE NONE None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[96m⏳ Evaluation Progress\u001b[0m\n",
      "Total Logs: 20\n",
      "Total Judgments: 39\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m⏳ Evaluation Progress\u001b[0m\n",
      "Total Logs: 20\n",
      "Total Judgments: 40\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96m📊 Evaluation Results for Evaluations SDK Demo/MedQA Answer Flow 8 \u001b[0m\n",
      "+----------------------------------+---------------------+\n",
      "|                                  |        Latest       |\n",
      "+----------------------------------+---------------------+\n",
      "|                           Run ID |        rEKHL        |\n",
      "+----------------------------------+---------------------+\n",
      "|                       Version ID |         None        |\n",
      "+----------------------------------+---------------------+\n",
      "|                            Added | 2024-11-11 14:27:49 |\n",
      "+----------------------------------+---------------------+\n",
      "|                       Evaluators |                     |\n",
      "+----------------------------------+---------------------+\n",
      "| Evaluations SDK Demo/Levenshtein |        22.85        |\n",
      "| Evaluations SDK Demo/Exact Match |         0.0         |\n",
      "+----------------------------------+---------------------+\n",
      "\n",
      "\n",
      "\n",
      "\u001b[96mView your Evaluation:\u001b[0m\n",
      "http://localhost:3000/project/fl_iiAjv1p4Kt7BUODEpsIro/evaluations/evr_Fe6hBqvelFS6mJCzmIqlw/stats\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanloop.evaluations.run(\n",
    "    file={\n",
    "        \"path\": \"Evaluations SDK Demo/MedQA Answer Flow 8\",\n",
    "        \"callable\": entrypoint,\n",
    "        \"version\": {\n",
    "            \"attributes\": {\n",
    "                \"prompt\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"environment\": \"evaluation\",\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"flow\",\n",
    "    },\n",
    "    name=\"MedQA Evaluation Decorators Debug\",\n",
    "    dataset={\n",
    "        \"datapoints\": datapoints,\n",
    "        \"path\": \"Evaluations SDK Demo/Dataset\",\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"path\": \"Evaluations SDK Demo/Levenshtein\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"number\",\n",
    "            \"callable\": compare_log_and_target,\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"Evaluations SDK Demo/Exact Match\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"boolean\",\n",
    "            \"callable\": exact_match,\n",
    "        },\n",
    "    ],\n",
    "    workers=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanloop-cookbook-XVtM0WJo-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
