{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Files declared in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial highlights how to combine Humanloop decorated files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from exact_match import exact_match\n",
    "from humanloop import Humanloop\n",
    "from levenshtein import compare_log_and_target\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"SDK/Callables Evaluation\"\n",
    "# DIRECTORY_PREFIX should lack a trailing slash\n",
    "if DIRECTORY_PREFIX := os.getenv(\"DIRECTORY_PREFIX\"):\n",
    "    DIRECTORY = f\"{DIRECTORY_PREFIX}/{DIRECTORY}\"\n",
    "print(DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiating the Humanloop client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiating the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = chromadb.chromadb.Client()\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# init collection into which we will add documents\n",
    "knowledge_base = pd.read_parquet(\"../../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(5, random_state=42)\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_df = pd.read_json(\"../../../assets/datapoints.jsonl\", lines=True)\n",
    "datapoints = [row.to_dict() for _, row in datapoints_df.iterrows()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Declare Humanloop Files via code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "- {option_A}\n",
    "- {option_B}\n",
    "- {option_C}\n",
    "- {option_D}\n",
    "- {option_E}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{retrieved_data}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "def ask_model(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    ") -> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(question)\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"option_A\": option_A,\n",
    "        \"option_B\": option_B,\n",
    "        \"option_C\": option_C,\n",
    "        \"option_D\": option_D,\n",
    "        \"option_E\": option_E,\n",
    "        \"retrieved_data\": retrieved_data,\n",
    "    }\n",
    "\n",
    "    # Populate the Prompt template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": TEMPLATE.format(**inputs),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def entrypoint(\n",
    "    question: str,\n",
    "    option_A: str,\n",
    "    option_B: str,\n",
    "    option_C: str,\n",
    "    option_D: str,\n",
    "    option_E: str,\n",
    "):\n",
    "    return ask_model(\n",
    "        question=question,\n",
    "        option_A=option_A,\n",
    "        option_B=option_B,\n",
    "        option_C=option_C,\n",
    "        option_D=option_D,\n",
    "        option_E=option_E,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanloop.evaluations.run(\n",
    "    file={\n",
    "        \"path\": f\"{DIRECTORY}/MedQA Answer Flow\",\n",
    "        \"callable\": entrypoint,\n",
    "        \"version\": {\n",
    "            \"attributes\": {\n",
    "                \"prompt\": {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"environment\": \"evaluation\",\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"flow\",\n",
    "    },\n",
    "    name=\"MedQA Evaluation Decorators\",\n",
    "    dataset={\n",
    "        \"datapoints\": datapoints,\n",
    "        \"path\": f\"{DIRECTORY}/Dataset\",\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"path\": f\"{DIRECTORY}/Levenshtein\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"number\",\n",
    "            \"callable\": compare_log_and_target,\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{DIRECTORY}/Exact Match\",\n",
    "            \"args_type\": \"target_required\",\n",
    "            \"return_type\": \"boolean\",\n",
    "            \"callable\": exact_match,\n",
    "        },\n",
    "    ],\n",
    "    workers=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
