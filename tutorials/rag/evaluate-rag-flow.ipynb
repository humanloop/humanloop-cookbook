{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b81e0e5be92583",
   "metadata": {},
   "source": [
    "# Humanloop RAG Evaluation Walkthrough\n",
    "\n",
    "The goal of this notebook is to demonstrate how to take an existing RAG pipeline and use Humanloop to evaluate it.\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Run an Eval on your RAG pipeline.\n",
    "2. Set up detailed logging with SDK decorators.\n",
    "3. Log to Humanloop manually\n",
    "\n",
    "\n",
    "## What is Humanloop?\n",
    "\n",
    "Humanloop is an interactive development environment designed to streamline the entire lifecycle of LLM app development. It serves as a central hub where AI, Product, and Engineering teams can collaborate on Prompt management, Evaluation and Monitoring workflows. \n",
    "\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "- **Retrieval** - Getting the relevant information from a larger data source for a given a query.\n",
    "- **Augmented** - Using the retrieved information as input to an LLM.\n",
    "- **Generation** - Generating an output from the model given the input.\n",
    "\n",
    "In practice, it remains an effective way to use LLMs for things like question answering, summarization, and more, where the data source is too large to fit in the context window of the LLM, or where providing the full data source for each query is not cost-effective.\n",
    "\n",
    "\n",
    "## What are the major challenges with RAG?\n",
    "\n",
    "Implementing RAG and other similar flows complicates the process of [Prompt Engineering](https://humanloop.com/blog/prompt-engineering-101) because you expand the design space of your application. There are lots of choices you need to make around the retrieval and Prompt components that can significantly impact the performance of your overall application. For example,\n",
    "- How do you select the data source?\n",
    "- How should it be chunked up and indexed?\n",
    "- What embedding and retrieval model should you use?\n",
    "- How should you combine the retrieved information with the query?\n",
    "- What should your system Prompt be? \n",
    "- Which model should you use?\n",
    "- What should your system message be?\n",
    "etc...\n",
    "\n",
    "The process of versioning, evaluating and monitoring your pipeline therefore needs to consider both the retrieval and generation components. This is where Humanloop can help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53965cd45425bab5",
   "metadata": {},
   "source": [
    "# Example RAG Pipeline\n",
    "\n",
    "We first need a reference RAG implementation. Our use case will be Q&A over a corpus of medical documents.\n",
    "\n",
    "- **Dataset**: we'll use a version of the [MedQA dataset](https://huggingface.co/dataset    s/bigbio/med_qa) from Hugging Face. This is a multiple-choice question-answering problem based on the United States Medical License Exams (USMLE), with reference textbooks that contain the required information to answer the questions.\n",
    "- **Retriever**: we're going to use [Chroma](https://docs.trychroma.com/getting-started) as a simple local vector DB with their default embedding model `all-MiniLM-L6-v2`. You can replace this with your favorite retrieval system.\n",
    "- **Prompt**: **the Prompt will be managed in code**, populated with the users question and the context retrieved from the Retriever and sent to [OpenAI](https://platform.openai.com/docs/api-reference/introduction) to generate the answer.\n",
    "\n",
    "\n",
    "### Where to store your Prompts?\n",
    "\n",
    "Generally speaking, when the engineering/applied AI teams are mainly responsible for managing the details of the Prompt, then the pattern of storing or constructing the Prompt in code works well. This is the pattern we follow in this tutorial. \n",
    "\n",
    "However, if the Product/Domain Expert teams are more involved in Prompt engineering and management, then the Prompt can instead be managed on Humanloop and retrieved or called by your code - this workflow lies outside the scope of this tutorial and we cover it separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da007e8494c60446",
   "metadata": {},
   "source": [
    "## Complete Prerequisites\n",
    "\n",
    "### Install packages\n",
    "\n",
    "This repository uses [Poetry](https://python-poetry.org/) to manage dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e3f74731b50ab",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!poetry install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f74709ea0177a8",
   "metadata": {},
   "source": [
    "### Initialise the SDKs\n",
    "\n",
    "You will need to set your OpenAI API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [OpenAI account](https://platform.openai.com/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ac94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dependencies\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from chromadb import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load .env file that contains API keys\n",
    "load_dotenv()\n",
    "\n",
    "# init clients\n",
    "chroma = chromadb.Client()\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ffee245036bc",
   "metadata": {},
   "source": [
    "### Set up the Vector DB\n",
    "This involves loading the data from the MedQA dataset and embedding the data within a collection in Chroma. This will take a couple of minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c5158d1d159535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection into which we will add documents\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# load knowledge base\n",
    "knowledge_base = pd.read_parquet(\"../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(10, random_state=42)\n",
    "\n",
    "\n",
    "# Add to Chroma - will by default use local vector DB and model all-MiniLM-L6-v2\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43484af0f2f10c",
   "metadata": {},
   "source": [
    "### Define the Prompt\n",
    "We define a simple prompt template that has variables for the question, answer options and retrieved data.\n",
    "\n",
    "It is generally good practise to define the Prompt details that impact the behaviour of the model in one place separate to your application logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187af8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "temperature = 0\n",
    "template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Options:\n",
    "- {{option_A}}\n",
    "- {{option_B}}\n",
    "- {{option_C}}\n",
    "- {{option_D}}\n",
    "- {{option_E}}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{{retrieved_data}}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "def populate_template(template: list, inputs: dict[str, str]) -> list:\n",
    "    \"\"\"Populate a template with input variables.\"\"\"\n",
    "    messages = []\n",
    "    for i, template_message in enumerate(template):\n",
    "        content = template_message[\"content\"]\n",
    "        for key, value in inputs.items():\n",
    "            content = content.replace(\"{{\" + key + \"}}\", value)\n",
    "        message = {**template_message, \"content\": content}\n",
    "        messages.append(message)\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86410ee6d885c4b",
   "metadata": {},
   "source": [
    "## Define the RAG Pipeline\n",
    "\n",
    "Now we provide the reference RAG pipeline using Chroma and OpenAI that takes a question and returns an answer. This is ultimately what we will evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c95ad9790ade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "def call_llm(**inputs):\n",
    "    # Populate the Prompt template\n",
    "    messages = populate_template(template, inputs)\n",
    "    \n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def ask_question(**inputs)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "\n",
    "    # Call LLM\n",
    "    return call_llm(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57f2c4b4533a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "chat_completion = ask_question(\n",
    "    **{\n",
    "        \"question\": \"A 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\",\n",
    "        \"option_A\": \"0%\",\n",
    "        \"option_B\": \"25%\",\n",
    "        \"option_C\": \"50%\",\n",
    "        \"option_D\": \"75%\",\n",
    "        \"option_E\": \"100%\",\n",
    "    }\n",
    ")\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58653e8",
   "metadata": {},
   "source": [
    "We now have a working RAG pipeline. We can now evaluate this pipeline using Humanloop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8d772",
   "metadata": {},
   "source": [
    "# Eval with Humanloop\n",
    "\n",
    "Now we will integrate Humanloop into our RAG pipeline to evaluate it. We will use the Humanloop SDK to run an Eval on our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d19844",
   "metadata": {},
   "source": [
    "## Initialise the SDK\n",
    "You will need to set your Humanloop API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [Humanloop organization](https://app.humanloop.com/account/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e442934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Humanloop SDK\n",
    "from humanloop import Humanloop\n",
    "\n",
    "load_dotenv()\n",
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"), base_url=os.getenv(\"HUMANLOOP_BASE_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89400c1f",
   "metadata": {},
   "source": [
    "## Set up Evaluators\n",
    "\n",
    "Here we will upload some Evaluators defined in code in `assets/evaluators/` so that Humanloop can manage running these for Evaluations (and later for Monitoring!)\n",
    "\n",
    "Alternatively you can define [AI](https://humanloop.com/docs/v5/guides/evals/llm-as-a-judge), [Code](https://humanloop.com/docs/v5/guides/evals/code-based-evaluator) and [Human-based](https://humanloop.com/docs/v5/guides/evals/human-evaluators) Evaluators via the UI.\n",
    "\n",
    "Furthermore, you can choose to not host the Evaluator on Humanloop and instead use your own runtime and post the results as part of the Evaluation. This can be useful for more complex workflows that require custom dependencies or resources. See the \"Running Evaluators locally\" section below for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b63765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_evaluators():\n",
    "    \"\"\"Uploads Evaluators to Humanloop.\n",
    "    \n",
    "    Uploads the \"Exact match\", \"Levenshtein\", and \"Reasoning\" Evaluators.\n",
    "    The \"Exact match\" and \"Levenshtein\" Evaluators are slight modifications to the examples\n",
    "    automatically created in the \"Example Evaluators\" folder in Humanloop when you signed up,\n",
    "    with some additional parsing for the output of this RAG pipeline.\n",
    "    \"\"\"\n",
    "    # Upload Code Evaluators\n",
    "    for evaluator_name, file_name, return_type in [\n",
    "        (\"Exact match\", \"exact_match.py\", \"boolean\"),\n",
    "        (\"Levenshtein\", \"levenshtein.py\", \"number\"),\n",
    "    ]:\n",
    "        with open(f\"../../assets/evaluators/{file_name}\", \"r\") as f:\n",
    "            code = f.read()\n",
    "        humanloop.evaluators.upsert(\n",
    "            path=f\"Evals demo/{evaluator_name}\",\n",
    "            spec={\n",
    "                \"evaluator_type\": \"python\",\n",
    "                \"arguments_type\": \"target_required\",\n",
    "                \"return_type\": return_type,\n",
    "                \"code\": code,\n",
    "            },\n",
    "            commit_message=f\"New version from {file_name}\",\n",
    "        )\n",
    "\n",
    "    # Upload an LLM Evaluator\n",
    "    humanloop.evaluators.upsert(\n",
    "        path=\"Evals demo/Reasoning\",\n",
    "        spec={\n",
    "            \"evaluator_type\": \"llm\",\n",
    "            \"arguments_type\": \"target_free\",\n",
    "            \"return_type\": \"boolean\",\n",
    "            \"prompt\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"endpoint\": \"complete\",\n",
    "                \"temperature\": 0,\n",
    "                \"template\": \"An answer is shown below. The answer contains 3 sections, separated by \\\"---\\\". The first section is the final answer. The second section is an explanation. The third section is a citation.\\n\\nEvaluate if the final answer follows from the citation and the reasoning in the explanation section. Give a brief explanation/discussion. Do not make your judgment based on factuality, but purely based on the logic presented.\\nOn a new line, give a final verdict of \\\"True\\\" or \\\"False\\\".\\n\\nAnswer:\\n{{log.output}}\",\n",
    "            },\n",
    "        },\n",
    "        commit_message=\"Initial reasoning evaluator.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90249e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_evaluators()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998aa66",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "Here we will create a Dataset on Humanloop using the MedQA test dataset. Alternatively you can create a data from Logs on Humanloop, or upload via the UI - see our [guide](https://humanloop.com/docs/v5/evaluation/guides/create-dataset). \n",
    "\n",
    "You can then effectively version control your Dataset centrally on Humanloop and hook into it for Evaluation workflows in code and via the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c0d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_humanloop():\n",
    "    df = pd.read_json(\"../../assets/datapoints.jsonl\", lines=True)\n",
    "\n",
    "    datapoints = [row.to_dict() for _i, row in df.iterrows()][0:20]\n",
    "    return humanloop.datasets.upsert(\n",
    "        path=\"Evals demo/MedQA test\",\n",
    "        datapoints=datapoints,\n",
    "        commit_message=f\"Added {len(datapoints)} datapoints from MedQA test dataset.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29aaeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = upload_dataset_to_humanloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62827499",
   "metadata": {},
   "source": [
    "### Evaluate the pipeline over the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247eb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_pager = humanloop.datasets.list_datapoints(dataset.id, version_id=dataset.version_id)\n",
    "datapoints = [datapoint for datapoint in datapoints_pager]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba72da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = humanloop.evaluations.run(\n",
    "    name=\"Demo cookbook\",\n",
    "    file={\n",
    "        \"path\": \"Evals demo/MedQA pipeline\",\n",
    "        \"callable\": ask_question,\n",
    "    },\n",
    "    dataset={\n",
    "        \"path\": \"Evals demo/MedQA test\",\n",
    "        \"datapoints\": datapoints,\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\"path\": \"Evals demo/Exact match\"},\n",
    "        {\"path\": \"Evals demo/Levenshtein\"},\n",
    "        {\"path\": \"Evals demo/Reasoning\"},\n",
    "        {\"path\": \"Example Evaluators/Code/Latency\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e7f0e",
   "metadata": {},
   "source": [
    "# Detailed logging\n",
    "\n",
    "So far, we've been treating the RAG pipeline like a black box and evaluating it as a whole.\n",
    "We can extend our logging to include more detailed information about the pipeline's internal steps. This can be useful for debugging and monitoring the various parts of the pipeline.\n",
    "\n",
    "We can do this by adding logging for the Prompt and Tool steps within the Flow using Humanloop's Python decorators. If you're using a different language, you can still log to Humanloop via the API. Skip to the \"Logging with the API\" section below or check out our [guide](https://humanloop.com/docs/v5/guides/observability/logging-through-api) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4c576",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "@humanloop.flow(path=\"Evals demo/MedQA pipeline\")\n",
    "def rag_pipeline(...):\n",
    "    ...\n",
    "\n",
    "@humanloop.tool(path=\"Evals demo/Retrieval tool\")\n",
    "def retrieval_tool(...):\n",
    "    ...\n",
    "\n",
    "@humanloop.prompt(path=\"Evals demo/LLM call\")\n",
    "def llm_call(...):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89690c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we give them a different name to keep the original functions around to allow for this cell to be run multiple times,\n",
    "# but you would not need to do so in your actual implementation.\n",
    "\n",
    "@humanloop.flow(path=\"Evals demo/MedQA pipeline\")\n",
    "def ask_question_decorated(**inputs: dict[str, str]):\n",
    "    retrieved_data = retrieval_tool_decorated(inputs[\"question\"])\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    return call_llm_decorated(**inputs)\n",
    "\n",
    "@humanloop.tool(path=\"Evals demo/Retrieval tool\")\n",
    "def retrieval_tool_decorated(question: str) -> str:\n",
    "    return retrieval_tool(question)\n",
    "\n",
    "@humanloop.prompt(path=\"Evals demo/LLM call\")\n",
    "def call_llm_decorated(**inputs):\n",
    "    return call_llm(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\n",
    "    ask_question_decorated(\n",
    "        **random.choice(datapoints).inputs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2061e",
   "metadata": {},
   "source": [
    "After running the above, you should see a new Log on Humanloop corresponding to the execution of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fbaf9",
   "metadata": {},
   "source": [
    "# Running Eval with decorators\n",
    "\n",
    "These decorated functions can similarly be used to run an Eval on the pipeline. This will allow you to evaluate the pipeline and see the detailed logs for each step in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c983abd",
   "metadata": {},
   "source": [
    "Let's change from `gpt-4o-mini` to `gpt-4o` and re-run the Eval.\n",
    "\n",
    "By passing in the same `name` to `humanloop.evaluations.run(...)` call, we'll add another run to the previously-created Evaluation on Humanloop. This will allow us to compare the two Runs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71580c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ed00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = humanloop.evaluations.run(\n",
    "    name=\"Demo cookbook\",\n",
    "    file={\n",
    "        \"path\": \"Evals demo/MedQA pipeline\",\n",
    "        \"callable\": ask_question_decorated,\n",
    "        \"type\": \"flow\",\n",
    "    },\n",
    "    dataset={\n",
    "        \"path\": \"Evals demo/MedQA test\",\n",
    "        \"datapoints\": datapoints,\n",
    "    },\n",
    "    evaluators=[\n",
    "        {\"path\": \"Evals demo/Exact match\"},\n",
    "        {\"path\": \"Evals demo/Levenshtein\"},\n",
    "        {\"path\": \"Evals demo/Reasoning\"},\n",
    "        {\"path\": \"Example Evaluators/Code/Latency\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4998d63",
   "metadata": {},
   "source": [
    "Viewing our Evaluation on Humanloop, we can see that our newly-added Run with `gpt-4o` has been added to the Evaluation.\n",
    "On the **Stats** tab, we can see that `gpt-4o` scores better for our \"Exact match\" (and \"Levenshtein\") metrics, but has higher latency.\n",
    "\n",
    "![Eval runs](../../assets/images/evaluate_rag_flow_stats.png)\n",
    "\n",
    "Perhaps surprisingly, `gpt-4o` performs worse according to our \"Reasoning\" Evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559c0f9",
   "metadata": {},
   "source": [
    "Humanloop also allows you to dive deeper into the specific Logs for each Run, to understand why the model performed the way it did.\n",
    "For example, looking closer at the \"Reasoning\" Evaluator in the above screenshot, `gpt-4o` performs worse according to our \"Reasoning\" Evaluator.\n",
    "\n",
    "Going to the **Review** tab and paging through Logs, we see that the \"Reasoning\" Evaluator has flagged the following `gpt-4o` Log, with the justification that it provided a citation that was not relevant to the question. `gpt-4o-mini` on the other hand did not provide any citation.\n",
    "\n",
    "![Eval runs](../../assets/images/evaluate_rag_flow_review.png)\n",
    "\n",
    "With Humanloop, you can measure the performance of your RAG pipelines and investigate changes in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761202e1",
   "metadata": {},
   "source": [
    "# Logging with the API\n",
    "\n",
    "Above, we've let the SDK handle logging and versioning for us. However, you can also log data to Humanloop using the API directly. This can be useful if you want to perform some post-processing on the data before logging it, or if you want to include additional metadata in the logs or versions.\n",
    "\n",
    "We'll now demonstrate how to extend your Humanloop logging with more fidelity; creating Tool, Prompt, and Flow Logs to give you full visibility.\n",
    "\n",
    "We add additional logging steps to our `ask_question` function to represent the full trace of events on Humanloop.\n",
    "\n",
    "(Note that the `run_id` and `source_datapoint_id` arguments are optional, and are included here for use in the Evaluation workflow demonstrated later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c08a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import inspect\n",
    "\n",
    "def ask_question_with_logging(run_id: str | None = None, source_datapoint_id: str | None = None, **inputs)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline.\"\"\"\n",
    "\n",
    "\n",
    "    trace = humanloop.flows.log(\n",
    "        path=\"evals_demo/medqa-flow\",\n",
    "        flow={\n",
    "            \"attributes\": {\n",
    "                \"prompt\": {\n",
    "                    \"template\": template,\n",
    "                    \"model\": model,\n",
    "                    \"temperature\": temperature,\n",
    "                },\n",
    "                \"tool\": {\n",
    "                    \"name\": \"retrieval_tool_v3\",\n",
    "                    \"description\": \"Retrieval tool for MedQA.\",\n",
    "                    \"source_code\": inspect.getsource(retrieval_tool),\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        start_time=datetime.now(),\n",
    "        run_id=run_id,\n",
    "        source_datapoint_id=source_datapoint_id,\n",
    "    )\n",
    "\n",
    "    # Retrieve context\n",
    "    start_time=datetime.now()\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "\n",
    "    # Log the retriever information to Humanloop separately\n",
    "    humanloop.tools.log(\n",
    "        path=\"Evals demo/Retrieval tool\",\n",
    "        tool={\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval_tool\",\n",
    "                \"description\": \"Retrieval tool for MedQA.\",\n",
    "            },\n",
    "            \"source_code\": inspect.getsource(retrieval_tool),\n",
    "        },\n",
    "        output=retrieved_data,\n",
    "        trace_parent_id=trace.id,\n",
    "        start_time=start_time,\n",
    "        end_time=datetime.now()\n",
    "    )\n",
    "    \n",
    "    # Populate the Prompt template\n",
    "    start_time=datetime.now()\n",
    "    messages = populate_template(template, inputs)\n",
    "    \n",
    "    # Call OpenAI to get response\n",
    "    chat_completion= openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    output = chat_completion.choices[0].message.content\n",
    "\n",
    "    # Log the prompt information to Humanloop separately\n",
    "    humanloop.prompts.log(\n",
    "        path=\"evals_demo/medqa-answer\",\n",
    "        prompt={\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"template\": template,\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        output=output,\n",
    "        output_message=chat_completion.choices[0].message,\n",
    "        trace_parent_id=trace.id,\n",
    "        start_time=start_time,\n",
    "        end_time=datetime.now()\n",
    "    )\n",
    "\n",
    "    # Close the trace\n",
    "    humanloop.flows.update_log(\n",
    "        log_id=trace.id,\n",
    "        output=output,\n",
    "        trace_status=\"complete\",\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ask_question_with_logging(\n",
    "        **random.choice(datapoints).inputs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde57f28",
   "metadata": {},
   "source": [
    "## Evaluating with manual logging\n",
    "\n",
    "To orchestrate your own Evaluations, you can pass in `run_id` and `source_datapoint_id` to the `humanloop.flows.log(...)` call to associate Logs with a specific Run and Datapoint.\n",
    "\n",
    "The following is an example of how you can manually create an Evaluation and Run, and log data to Humanloop using the API,\n",
    "giving you full control over the Evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "904fd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation\n",
    "evaluation = humanloop.evaluations.create(\n",
    "    name=\"Manual logging demo\",\n",
    "    file={\"path\": \"Evals demo/MedQA pipeline\"},\n",
    "    evaluators=[\n",
    "        {\"path\": \"Evals demo/Exact match\"},\n",
    "        {\"path\": \"Evals demo/Levenshtein\"},\n",
    "        {\"path\": \"Evals demo/Reasoning\"},\n",
    "        {\"path\": \"Example Evaluators/Code/Latency\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create Run\n",
    "run = humanloop.evaluations.create_run(id=evaluation.id, dataset={\"path\": \"Evals demo/MedQA test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Run the pipeline over the Dataset\n",
    "for datapoint in tqdm(datapoints):\n",
    "    ask_question_with_logging(run_id=run.id, source_datapoint_id=datapoint.id, **datapoint.inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3ea11",
   "metadata": {},
   "source": [
    "You can then similarly view results on the Humanloop UI.\n",
    "\n",
    "![Eval Logs table](../../assets/images/evaluate_rag_flow_logs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
