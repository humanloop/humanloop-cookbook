{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Humanloop RAG Evaluation Walkthrough\n",
    "The goal of this notebook is to demonstrate how to take an existing RAG pipeline and integrate Humanloop in order to:\n",
    "1. Manage your [Prompts](https://humanloop.com/docs/v5/concepts/prompts) in code but be able to version and run them on Humanloop\n",
    "2. Setup logging for both your retriever [Tool](https://humanloop.com/docs/v5/concepts/prompts) managed in code and your Prompt managed on Humanloop\n",
    "3. Create a [Dataset](https://humanloop.com/docs/v5/concepts/prompts) and run Evaluations to benchmark the performance of your RAG pipeline\n",
    "4. Configure [Evaluators](https://humanloop.com/docs/v5/concepts/evaluators) for monitoring your RAG pipeline in production\n",
    "\n",
    "\n",
    "## What is Humanloop?\n",
    "Humanloop is an interactive development environment designed to streamline the entire lifecycle of LLM app development. It serves as a central hub where AI, Product, and Engineering teams can collaborate on Prompt management, Evaluation and Monitoring workflows. \n",
    "\n",
    "\n",
    "## What is RAG?\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "- **Retrieval** - Getting the relevant information from a larger data source for a given a query.\n",
    "- **Augmented** - Using the retrieved information as input to an LLM.\n",
    "- **Generation** - Generating an output from the model given the input.\n",
    "\n",
    "In practise, it remains an effective way to exploit LLMs for things like question answering, summarization, and more, where the data source is too large to fit in the context window of the LLM, or where providing the full data source for each query is not cost-effective.\n",
    "\n",
    "\n",
    "## What are the major challenges with RAG?\n",
    "Implementing RAG and other similar flows complicates the process of [Prompt Engineering](https://humanloop.com/blog/prompt-engineering-101) because you expand the design space of your application. There are lots of choices you need to make around the retrieval component that can significantly impact the performance of your overall application. For example,\n",
    "- How do you select the data source?\n",
    "- How should it be chunked up and indexed?\n",
    "- What embedding and retrieval model should you use?\n",
    "- How should you combine the retrieved information with the query?\n",
    "- What should your system Prompt be? \n",
    "\n",
    "The process of versioning, evaluating and monitoring your pipeline therefore needs to consider both the retrieval and generation components. This is where Humanloop can help.\n"
   ],
   "id": "c5b81e0e5be92583"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cf4a5f234d7bac09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example RAG Pipeline\n",
    "\n",
    "We first need a reference RAG implementation. Our use case will be Q&A over medical docs - leveraging the [MedQA dataset](https://huggingface.co/datasets/bigbio/med_qa) from Hugging Face.\n",
    "\n",
    "We're going to use [Chroma](https://docs.trychroma.com/getting-started) as a simple local vector DB. You can replace this with your favorite retrieval system.\n",
    "\n",
    "\n"
   ],
   "id": "53965cd45425bab5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-requisites ",
   "id": "da007e8494c60446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T11:20:18.947373Z",
     "start_time": "2024-08-21T11:20:14.779075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --quiet chromadb\n",
    "!pip install --quiet openai\n",
    "!pip install --quiet humanloop==0.8.0b6"
   ],
   "id": "5b3e3f74731b50ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T11:25:30.238654Z",
     "start_time": "2024-08-21T11:25:29.934053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up dependencies for reference implementation\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from chromadb import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_KEY = \"\"\n",
    "\n",
    "# init clients\n",
    "chroma = chromadb.Client()\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "# init collection into which we will add documents\n",
    "collection = chroma.create_collection(name=\"MedQA\")\n",
    "\n",
    "# load dataset\n",
    "# TODO: LOAD MEDQA DATASET HERE\n",
    "\n",
    "# Add to Chroma - will by default use local vector DB and model all-MiniLM-L6-v2\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n"
   ],
   "id": "15c5158d1d159535",
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection MedQA already exists",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUniqueConstraintError\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m openai \u001B[38;5;241m=\u001B[39m OpenAI(api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# init collection into which we will add documents\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m collection \u001B[38;5;241m=\u001B[39m \u001B[43mchroma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_collection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMedQA\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# load dataset\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# TODO: LOAD MEDQA DATASET HERE\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Add to Chroma - will by default use local vector DB and model all-MiniLM-L6-v2\u001B[39;00m\n\u001B[1;32m     22\u001B[0m collection\u001B[38;5;241m.\u001B[39madd(\n\u001B[1;32m     23\u001B[0m     documents\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     24\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a document about pineapple\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m     ids\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid2\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     28\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/chromadb/api/client.py:117\u001B[0m, in \u001B[0;36mClient.create_collection\u001B[0;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_collection\u001B[39m(\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    115\u001B[0m     get_or_create: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    116\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Collection:\n\u001B[0;32m--> 117\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_collection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtenant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtenant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatabase\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mget_or_create\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_or_create\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfiguration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Collection(\n\u001B[1;32m    126\u001B[0m         client\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_server,\n\u001B[1;32m    127\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    128\u001B[0m         embedding_function\u001B[38;5;241m=\u001B[39membedding_function,\n\u001B[1;32m    129\u001B[0m         data_loader\u001B[38;5;241m=\u001B[39mdata_loader,\n\u001B[1;32m    130\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m tracer, granularity\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace_granularity \u001B[38;5;241m<\u001B[39m granularity:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracer:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/chromadb/api/segment.py:176\u001B[0m, in \u001B[0;36mSegmentAPI.create_collection\u001B[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001B[0m\n\u001B[1;32m    164\u001B[0m model \u001B[38;5;241m=\u001B[39m CollectionModel(\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[1;32m    166\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    173\u001B[0m     dimension\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    174\u001B[0m )\n\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001B[39;00m\n\u001B[0;32m--> 176\u001B[0m coll, created \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sysdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_collection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfiguration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_configuration\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdimension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# This is lazily populated on the first add\u001B[39;49;00m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mget_or_create\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_or_create\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtenant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtenant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatabase\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m tracer, granularity\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace_granularity \u001B[38;5;241m<\u001B[39m granularity:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracer:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/chromadb/db/mixins/sysdb.py:227\u001B[0m, in \u001B[0;36mSqlSysDB.create_collection\u001B[0;34m(self, id, name, configuration, metadata, dimension, get_or_create, tenant, database)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    221\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_collections(\n\u001B[1;32m    222\u001B[0m                 \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39mcollection[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m], tenant\u001B[38;5;241m=\u001B[39mtenant, database\u001B[38;5;241m=\u001B[39mdatabase\n\u001B[1;32m    223\u001B[0m             )[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    224\u001B[0m             \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    225\u001B[0m         )\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 227\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m UniqueConstraintError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCollection \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already exists\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    229\u001B[0m collection \u001B[38;5;241m=\u001B[39m Collection(\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[1;32m    231\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    237\u001B[0m     version\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    238\u001B[0m )\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtx() \u001B[38;5;28;01mas\u001B[39;00m cur:\n",
      "\u001B[0;31mUniqueConstraintError\u001B[0m: Collection MedQA already exists"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T11:28:32.324148Z",
     "start_time": "2024-08-21T11:28:32.319320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reference RAG pipeline using Chroma and OpenAI\n",
    "\n",
    "def ask_question(question: str)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    response = collection.query(query_texts=[\"apple\"], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    \n",
    "\n",
    "    # Generate answer\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant. Here is some context: {retrieved_doc}.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    print(messages)\n",
    "    answer = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages\n",
    "    ).choices[0].message.content\n",
    "    return answer"
   ],
   "id": "53c95ad9790ade59",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T11:28:40.883975Z",
     "start_time": "2024-08-21T11:28:37.778978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the pipeline\n",
    "\n",
    "print(ask_question(\"What is a pineapple?\"))"
   ],
   "id": "da57f2c4b4533a09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistant. Here is some context: This is a document about pineapple.'}, {'role': 'user', 'content': 'What is a pineapple?'}]\n",
      "A pineapple is a tropical fruit with a tough, spiky exterior and sweet, juicy, yellow flesh inside. It is known for its unique appearance, with a globe-like shape covered in rough, diamond-shaped scales and topped with a crown of stiff, green leaves. Pineapples are rich in vitamins, enzymes, and antioxidants, making them a popular fruit for their health benefits. They can be eaten fresh, cooked, juiced, or preserved and are used in a variety of dishes and beverages around the world. The scientific name for the pineapple is *Ananas comosus*, and it belongs to the bromeliad family. Pineapples are also noted for containing bromelain, an enzyme that can aid in digestion.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Humanloop Integration\n",
    "\n",
    "The steps to the Humanloop integration are as follows:\n",
    "....\n",
    "\n",
    "We demonstrate how you can log to or call any of the core entities on Humanloop "
   ],
   "id": "f9fc3cffe1b5cb99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Manage your Prompt in code\n",
    "\n",
    "def ask_question(question: str)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    response = collection.query(query_texts=[\"question\"], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    \n",
    "    # Generate answer using Prompt managed on Humanloop\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": retrieved_doc}\n",
    "        ]\n",
    "    answer = hl.prompt.call(\n",
    "        path=\"faq-bot/rag-prompt\",\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=messages,\n",
    "        messages=messages,\n",
    "        temperature=0.5\n",
    "        \n",
    "    )\n",
    "    return answer"
   ],
   "id": "985a6fa7b2f095da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Manage your Prompt on Humanloop\n",
    "\n",
    "def ask_question(question: str)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    response = collection.query(query_texts=[\"question\"], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    \n",
    "    # Generate answer using Prompt managed on Humanloop\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": retrieved_doc}\n",
    "        ]\n",
    "    answer = hl.prompt.call(\n",
    "        path=\"faq-bot/rag-prompt\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return answer"
   ],
   "id": "4571b0ecf202147b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Log your tool calls too\n",
    "\n",
    "def ask_question(question: str)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    response = collection.query(query_texts=[\"question\"], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    \n",
    "    # log tool to Humanloop \n",
    "    hl.tool.log(\n",
    "        path=\"faq-bot/rag-retriever\",\n",
    "        query=question,\n",
    "        retrieved_doc=retrieved_doc\n",
    "    )\n",
    "    \n",
    "    # Generate answer using Prompt managed on Humanloop\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": retrieved_doc}\n",
    "        ]\n",
    "    answer = hl.prompt.call(\n",
    "        path=\"faq-bot/rag-prompt\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return answer"
   ],
   "id": "340d1b2b8b89b86f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up Evaluations\n",
    "\n",
    "\n",
    "## Creating a dataset\n",
    "- From your existing logs\n",
    "\n",
    "- using the SDK "
   ],
   "id": "595e5f31dc65ba72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
