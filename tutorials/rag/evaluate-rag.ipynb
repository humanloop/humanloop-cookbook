{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b81e0e5be92583",
   "metadata": {},
   "source": [
    "# Humanloop RAG Evaluation Walkthrough\n",
    "The goal of this notebook is to demonstrate how to take an existing RAG pipeline and integrate Humanloop in order to:\n",
    "1. Setup logging for both your [Prompt](https://humanloop.com/docs/v5/concepts/prompts) and retriever [Tool](https://humanloop.com/docs/v5/concepts/prompts) so that you can easily track the versions of these components.\n",
    "2. Create a [Dataset](https://humanloop.com/docs/v5/concepts/prompts) and run Evaluations to benchmark the performance of your RAG pipeline.\n",
    "3. Configure [Evaluators](https://humanloop.com/docs/v5/concepts/evaluators) for monitoring your RAG pipeline in production.\n",
    "\n",
    "\n",
    "## What is Humanloop?\n",
    "Humanloop is an interactive development environment designed to streamline the entire lifecycle of LLM app development. It serves as a central hub where AI, Product, and Engineering teams can collaborate on Prompt management, Evaluation and Monitoring workflows. \n",
    "\n",
    "\n",
    "## What is RAG?\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "- **Retrieval** - Getting the relevant information from a larger data source for a given a query.\n",
    "- **Augmented** - Using the retrieved information as input to an LLM.\n",
    "- **Generation** - Generating an output from the model given the input.\n",
    "\n",
    "In practise, it remains an effective way to exploit LLMs for things like question answering, summarization, and more, where the data source is too large to fit in the context window of the LLM, or where providing the full data source for each query is not cost-effective.\n",
    "\n",
    "\n",
    "## What are the major challenges with RAG?\n",
    "Implementing RAG and other similar flows complicates the process of [Prompt Engineering](https://humanloop.com/blog/prompt-engineering-101) because you expand the design space of your application. There are lots of choices you need to make around the retrieval and Prompt components that can significantly impact the performance of your overall application. For example,\n",
    "- How do you select the data source?\n",
    "- How should it be chunked up and indexed?\n",
    "- What embedding and retrieval model should you use?\n",
    "- How should you combine the retrieved information with the query?\n",
    "- What should your system Prompt be? \n",
    "- Which model should you use?\n",
    "- What should your system message be?\n",
    "etc...\n",
    "\n",
    "The process of versioning, evaluating and monitoring your pipeline therefore needs to consider both the retrieval and generation components. This is where Humanloop can help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a5f234d7bac09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53965cd45425bab5",
   "metadata": {},
   "source": [
    "# Example RAG Pipeline\n",
    "\n",
    "We first need a reference RAG implementation. Our use case will be Q&A over a corpus of medical documents.\n",
    "\n",
    "- **Dataset**: we'll use a version of the [MedQA dataset](https://huggingface.co/datasets/bigbio/med_qa) from Hugging Face. This is a multiple choice question answering problem based on the United States Medical License Exams (USMLE), with reference text books that contain the required information to answer the questions.\n",
    "- **Retriever**: we're going to use [Chroma](https://docs.trychroma.com/getting-started) as a simple local vector DB with their default embedding model `all-MiniLM-L6-v2`. You can replace this with your favorite retrieval system.\n",
    "- **Prompt**: **the Prompt will be managed in code**, populated with the users question and the context retrieved from the Retriever and sent to [OpenAI](https://platform.openai.com/docs/api-reference/introduction) to generate the answer.\n",
    "\n",
    "### Where to store your Prompts?\n",
    "\n",
    "Generally speaking, when the engineering/applied AI teams are mainly responsible for managing the details of the Prompt, then the pattern of storing or constructing the Prompt in code works well. This is the pattern we follow in this tutorial. \n",
    "\n",
    "However, if the Product/Domain Expert teams are more involved in Prompt engineering and management, then the Prompt can instead be managed on Humanloop and retrieved or called by your code - this workflow lies outside the scope of this tutorial and we cover it separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da007e8494c60446",
   "metadata": {},
   "source": [
    "## Complete Prerequisites\n",
    "\n",
    "### Install packages\n",
    "We use poetry to manage dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b3e3f74731b50ab",
   "metadata": {},
   "source": "!poetry install",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialise the SDKs\n",
    "\n",
    "You will need to set your OpenAI API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [OpenAI account](https://platform.openai.com/api-keys).\n"
   ],
   "id": "80f74709ea0177a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:33:50.146506Z",
     "start_time": "2024-08-23T15:33:47.998645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up dependencies\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from chromadb import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load .env file that contains API keys\n",
    "load_dotenv()\n",
    "\n",
    "# init clients\n",
    "chroma = chromadb.Client()\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n"
   ],
   "id": "47ac94aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set up the Vector DB\n",
    "This involves loading the data from the MedQA dataset and embedding the data within a collection in Chroma. This will take a couple of minutes to complete."
   ],
   "id": "f74ffee245036bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:34:00.704314Z",
     "start_time": "2024-08-23T15:33:58.177868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init collection into which we will add documents\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# load knowledge base\n",
    "knowledge_base = pd.read_parquet(\"../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(5, random_state=42)\n",
    "\n",
    "\n",
    "# Add to Chroma - will by default use local vector DB and model all-MiniLM-L6-v2\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ],
   "id": "15c5158d1d159535",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the Prompt\n",
    "We define a simple prompt template that has variables for the question, answer options and retrieved data.\n",
    "\n",
    "It is generally good practise to define the Prompt details that impact the behaviour of the model in one place separate to your application logic."
   ],
   "id": "cd43484af0f2f10c"
  },
  {
   "cell_type": "code",
   "id": "187af8c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:34:03.211116Z",
     "start_time": "2024-08-23T15:34:03.206670Z"
    }
   },
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0\n",
    "template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Options:\n",
    "- {{option_A}}\n",
    "- {{option_B}}\n",
    "- {{option_C}}\n",
    "- {{option_D}}\n",
    "- {{option_E}}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{{retrieved_data}}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "def populate_template(template: list, inputs: dict[str, str]) -> list:\n",
    "    \"\"\"Populate a template with input variables.\"\"\"\n",
    "    messages = []\n",
    "    for i, template_message in enumerate(template):\n",
    "        content = template_message[\"content\"]\n",
    "        for key, value in inputs.items():\n",
    "            content = content.replace(\"{{\" + key + \"}}\", value)\n",
    "        message = {**template_message, \"content\": content}\n",
    "        messages.append(message)\n",
    "    return messages\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define the RAG Pipeline\n",
    "\n",
    "Now we provide the reference RAG pipeline using Chroma and OpenAI that takes a question and returns an answer. This is ultimately what we will evaluate.\n"
   ],
   "id": "e86410ee6d885c4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:34:08.622675Z",
     "start_time": "2024-08-23T15:34:08.618270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict[str, str])-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    \n",
    "    # Populate the Prompt template\n",
    "    messages = populate_template(template, inputs)\n",
    "    \n",
    "    # Call OpenAI to get response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "    return answer"
   ],
   "id": "53c95ad9790ade59",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "da57f2c4b4533a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:34:13.119142Z",
     "start_time": "2024-08-23T15:34:11.110958Z"
    }
   },
   "source": [
    "# Test the pipeline\n",
    "\n",
    "print(\n",
    "    ask_question(\n",
    "        {\n",
    "            \"question\": \"A 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\",\n",
    "            'option_A': '0%', 'option_B': '25%', 'option_C': '50%', 'option_D': '75%', 'option_E': '100%'\n",
    "        }\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "50%\n",
      "---\n",
      "The probability that the second daughter is a carrier of the disease is 50%. This is because the daughters of a male with hemophilia A will either inherit the disease (if they receive the affected X chromosome) or be carriers (if they receive the normal X chromosome). Since the daughters are unaffected, they must be carriers of the disease.\n",
      "\n",
      "---\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "f9fc3cffe1b5cb99",
   "metadata": {},
   "source": [
    "# Humanloop Integration\n",
    "\n",
    "We now integrate Humanloop into the RAG pipeline to first enable logging and then to trigger evaluations against a dataset.\n",
    "\n",
    "\n",
    "## Initialise the SDK\n",
    "You will need to set your Humanloop API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [Humanloop organization](https://app.humanloop.com/account/api-keys).\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:35:38.521962Z",
     "start_time": "2024-08-23T15:35:38.505454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Init the Humanloop SDK\n",
    "from humanloop import Humanloop\n",
    "\n",
    "load_dotenv()\n",
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"), base_url=\"https://neostaging.humanloop.ml/v5\")"
   ],
   "id": "6c2102bcad49c932",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "f6d598fd",
   "metadata": {},
   "source": [
    "## Integrate Logging\n",
    "\n",
    "Below, we add a `humanloop.tools.log(...)` call after the retrieval step to log the retrieved documents to Humanloop\n",
    "and a `humanloop.prompts.log(...)` call after the chat completion generation.\n",
    "We also pass in a `session_id` to link these two Logs together.\n",
    "\n",
    "On running this updated code, Humanloop will now begin to track the versions of your Tool and Prompt and their inputs, outputs and associated metadata. "
   ]
  },
  {
   "cell_type": "code",
   "id": "985a6fa7b2f095da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:36:30.751114Z",
     "start_time": "2024-08-23T15:36:30.747015Z"
    }
   },
   "source": [
    "# redefine the ask_question function to include logging\n",
    "import uuid\n",
    "import inspect\n",
    "\n",
    "def ask_question(inputs: dict[str, str])-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "    \n",
    "    # Log the context and retriever details to your Humanloop Tool\n",
    "    session_id = uuid.uuid4().hex\n",
    "    humanloop.tools.log(\n",
    "        path=\"evals_demo/medqa-retrieval\",\n",
    "        tool={\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval_tool\",\n",
    "                \"description\": \"Retrieval tool for MedQA.\",\n",
    "            },\n",
    "            \"source_code\": inspect.getsource(retrieval_tool),\n",
    "        },\n",
    "        output=retrieved_data,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    \n",
    "    # Populate the Prompt template\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    messages = populate_template(template, inputs)\n",
    "    \n",
    "    # Call OpenAI to get a response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "    \n",
    "    # Log the response and Prompt details to your Humanloop Prompt\n",
    "    humanloop.prompts.log(\n",
    "        path=\"evals_demo/medqa-answer\",\n",
    "        prompt={\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"template\": template,\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        output=chat_completion.choices[0].message.content,\n",
    "        output_message=chat_completion.choices[0].message,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    return answer"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:36:36.490252Z",
     "start_time": "2024-08-23T15:36:33.346891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the pipeline\n",
    "\n",
    "print(\n",
    "    ask_question(\n",
    "        {\n",
    "            \"question\": \"A 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\",\n",
    "            'option_A': '0%', 'option_B': '25%', 'option_C': '50%', 'option_D': '75%', 'option_E': '100%'\n",
    "        }\n",
    "    )\n",
    ")"
   ],
   "id": "acfe1f5d648b980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "50%\n",
      "---\n",
      "Each daughter of a male with hemophilia A has a 50% chance of being a carrier of the disease. In this case, since the daughters are unaffected, the second daughter also has a 50% chance of being a carrier.\n",
      "---\n",
      "\"InternalMed_Harrison. Table 149-2 lists the currently recommended drugs of choice for prophylaxis of malaria, by destination.\"\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Check your Humanloop workspace\n",
    "\n",
    "After running this pipeline, you will now see your Prompt and Tool logs in your Humanloop workspace:\n",
    "\n",
    "If you make changes to your Prompt in code and re-run the pipeline, you will a new version of the Prompt created in Humanloop:\n",
    "\n",
    "\n",
    "![Prompt Logs](../../assets/images/prompt_version.png)\n",
    "\n"
   ],
   "id": "696b87c33a608ed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffd51a498d9584fa"
  },
  {
   "cell_type": "markdown",
   "id": "595e5f31dc65ba72",
   "metadata": {},
   "source": [
    "# Triggering Evaluations\n",
    "\n",
    "We will now extend our implementation to allow us to run Evaluations on Humanloop against a specific test dataset.\n",
    "\n",
    "This involves the following steps:\n",
    "1. Extend our logging to include info needed by Evaluations.\n",
    "2. Create a Dataset that we can manage and re-use on Humanloop as the source of truth.\n",
    "3. Create some Evaluators that we can manage and re-use on Humanloop that can provide judgements on the performance of our Pipeline.\n",
    "4. Trigger an Evaluation and view the results.\n",
    "\n",
    "Then as you tweak your pipeline in code, this will allow you to easily track and compare the performance of different versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272f102",
   "metadata": {},
   "source": [
    "## Extend logging\n",
    "\n",
    "Add `source_datapoint_id` and `evaluation_id` to the `humanloop.prompt.log(...)` and `humanloop.tool.log(...)` calls.\n",
    "We do this below by adding the optional `datapoint_id` argument to `ask_question(...)`."
   ]
  },
  {
   "cell_type": "code",
   "id": "d208b64c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:42:02.771054Z",
     "start_time": "2024-08-23T15:42:02.761652Z"
    }
   },
   "source": [
    "import inspect\n",
    "import uuid\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict[str, str], datapoint_id: str | None = None, evaluation_id: str| None = None)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "    \n",
    "    # Log the context and retriever details to your Humanloop Tool\n",
    "    session_id = uuid.uuid4().hex\n",
    "    humanloop.tools.log(\n",
    "        path=\"evals_demo/medqa-retrieval\",\n",
    "        tool={\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval_tool\",\n",
    "                \"description\": \"Retrieval tool for MedQA.\",\n",
    "            },\n",
    "            \"source_code\": inspect.getsource(retrieval_tool),\n",
    "        },\n",
    "        output=retrieved_data,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    \n",
    "    # Populate the Prompt template\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    messages = populate_template(template, inputs)\n",
    "    \n",
    "    # Call OpenAI to get a response\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "    \n",
    "    # Log the response and Prompt details to your Humanloop Prompt\n",
    "    humanloop.prompts.log(\n",
    "        path=\"evals_demo/medqa-answer\",\n",
    "        prompt={\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"template\": template,\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        output=chat_completion.choices[0].message.content,\n",
    "        output_message=chat_completion.choices[0].message,\n",
    "        session_id=session_id,\n",
    "        # NB: New arguments to link to Evaluation and Dataset\n",
    "        source_datapoint_id=datapoint_id,\n",
    "        evaluation_id=evaluation_id,\n",
    "    )\n",
    "\n",
    "    return answer"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "78969c2f",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "Here we will create a Dataset on Humanloop using the MedQA test dataset. Alternatively you can create a data from Logs on Humanloop, or upload via the UI - see our [guide](https://humanloop.com/docs/v5/evaluation/guides/create-dataset). \n",
    "\n",
    "You can then effectively version control your Dataset centrally on Humanloop and hook into it for Evaluation workflows in code and via the UI."
   ]
  },
  {
   "cell_type": "code",
   "id": "87c70e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:37:34.825719Z",
     "start_time": "2024-08-23T15:37:34.822080Z"
    }
   },
   "source": [
    "def upload_dataset_to_humanloop():\n",
    "    df = pd.read_json(\"../../assets/datapoints.jsonl\", lines=True)\n",
    "\n",
    "    datapoints = [row.to_dict() for _i, row in df.iterrows()][0:20]\n",
    "    return humanloop.datasets.upsert(\n",
    "        path=\"evals_demo/medqa-test\",\n",
    "        datapoints=datapoints,\n",
    "        commit_message=f\"Added {len(datapoints)} datapoints from MedQA test dataset.\",\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "deae78c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:37:45.990135Z",
     "start_time": "2024-08-23T15:37:45.128691Z"
    }
   },
   "source": "dataset = upload_dataset_to_humanloop()",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "3ffb5c76",
   "metadata": {},
   "source": [
    "## Set up Evaluators\n",
    "\n",
    "Here we will upload some Evaluators defined in code in `assets/evaluators/` so that Humanloop can manage running these for Evaluations (and later for Monitoring!)\n",
    "\n",
    "Alternatively you can define AI, Code and Human based Evaluators via the UI - see the relevant `How-to guides` on [Evaluations](https://humanloop.com/docs/v5/evaluation/overview) for creating Evaluators of different kinds.\n",
    "\n",
    "Further you can choose to not host the Evaluator on Humanloop and instead use your own runtime and instead post the results as part of the Evaluation. This can be useful for more complex workflows that require custom dependencies or resources, but lies outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "id": "0dcb9069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:38:15.556693Z",
     "start_time": "2024-08-23T15:38:15.550938Z"
    }
   },
   "source": [
    "def upload_evaluators():\n",
    "    for evaluator_name, return_type in [\n",
    "        (\"exact_match\", \"boolean\"),\n",
    "        (\"levenshtein\", \"number\"),\n",
    "    ]:\n",
    "        with open(f\"../../assets/evaluators/{evaluator_name}.py\", \"r\") as f:\n",
    "            code = f.read()\n",
    "        humanloop.evaluators.upsert(\n",
    "            path=f\"evals_demo/{evaluator_name}\",\n",
    "            spec={\n",
    "                \"evaluator_type\": \"python\",\n",
    "                \"arguments_type\": \"target_required\",\n",
    "                \"return_type\": return_type,\n",
    "                \"code\": code,\n",
    "            },\n",
    "            commit_message=f\"New version from {evaluator_name}.py\",\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "8138e15d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:38:18.417812Z",
     "start_time": "2024-08-23T15:38:17.718420Z"
    }
   },
   "source": [
    "upload_evaluators()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "8b48f645",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can start to trigger Evaluations on Humanloop using our Dataset and Evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "id": "12405c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:42:07.923189Z",
     "start_time": "2024-08-23T15:42:07.555341Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Create the Evaluation specifying the Dataset and Evaluators to use\n",
    "evaluation = humanloop.evaluations.create(\n",
    "    # NB: you can also use the `id` to reference Datasets and Evaluators \n",
    "    dataset={\"path\": \"evals_demo/medqa-test\"},\n",
    "    evaluators=[\n",
    "        {\"path\": \"evals_demo/exact_match\"},\n",
    "        {\"path\": \"evals_demo/levenshtein\"},\n",
    "    ],\n",
    ")\n",
    "print(f\"Evaluation created: {evaluation.id}\")\n",
    "\n",
    "def populate_evaluation():\n",
    "    \"\"\"Run a variation of your Pipeline over the Dataset to populate results\"\"\"\n",
    "    retrieved_dataset = humanloop.datasets.get(\n",
    "        id=evaluation.dataset.id,\n",
    "        include_datapoints=True,\n",
    "    )\n",
    "    for datapoint in tqdm(retrieved_dataset.datapoints):\n",
    "        ask_question(\n",
    "            inputs=datapoint.inputs,\n",
    "            datapoint_id=datapoint.id,\n",
    "            evaluation_id=evaluation.id,\n",
    "        )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation created: evr_z0uhjU1QQp1IcjolUAhBI\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "c951317c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:45:14.574155Z",
     "start_time": "2024-08-23T15:42:14.188970Z"
    }
   },
   "source": [
    "populate_evaluation()\n",
    "\n",
    "# Then change your pipeline and run this function again to populate additional columns in your Evaluation!"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [03:00<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Get Results and URL\n",
    "We can not get the aggregate results via the API and the URL to navigate to the Evaluation in the Humanloop UI."
   ],
   "id": "a4dd1e7b7e542c45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T15:46:05.297135Z",
     "start_time": "2024-08-23T15:46:05.061228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation = humanloop.evaluations.get(id=evaluation.id)\n",
    "print(\"URL: \", evaluation.url)"
   ],
   "id": "e22e0558dc082de6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationResponse(id='evr_z0uhjU1QQp1IcjolUAhBI', dataset=DatasetResponse(path='evals_demo/medqa-test', id='ds_8liIEsObN5SAIq0c6aNm2', directory_id='dir_WFiBDkGinaPccJigBSHQ1', name='medqa-test', version_id='dsv_F2RTWrcSO2e1ZbrcktFXU', type='dataset', environments=[EnvironmentResponse(id='env_NakuPkQXr8w4dYkTAAynO', created_at=datetime.datetime(2024, 4, 29, 8, 1, 19, 415384), name='production', tag='default')], created_at=datetime.datetime(2024, 8, 23, 15, 37, 45, 542145), updated_at=datetime.datetime(2024, 8, 23, 15, 37, 45, 542145), created_by={'id': 'usr_iBV0KNZkHdwXZSIUSaMKG', 'email_address': 'pnhayes+@tcd.ie', 'full_name': 'Peter', 'platform_access': 'user'}, status='committed', last_used_at=datetime.datetime(2024, 8, 23, 15, 37, 45, 542145), commit_message='Added 50 datapoints from MedQA test dataset.', datapoints_count=50, datapoints=None, team_id='tm_b79syTwUvFjr0T1tmT8wq'), evaluatees=[EvaluateeResponse(version={'path': 'evals_demo/medqa-answer', 'id': 'pr_W0F7hDRYvtvRZywP98dL5', 'directory_id': 'dir_WFiBDkGinaPccJigBSHQ1', 'model': 'gpt-3.5-turbo', 'endpoint': 'chat', 'template': [{'content': 'Answer the following question factually.\\n\\nQuestion: {{question}}\\n\\nOptions:\\n- {{option_A}}\\n- {{option_B}}\\n- {{option_C}}\\n- {{option_D}}\\n- {{option_E}}\\n\\n---\\n\\nHere is some retrieved information that might be helpful.\\nRetrieved data:\\n{{retrieved_data}}\\n\\n---\\n\\nGive you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\\n```\\n<chosen option verbatim>\\n---\\n<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\\n---\\n<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\\n```\\n', 'name': None, 'tool_call_id': None, 'role': 'system', 'tool_calls': None}], 'provider': 'openai', 'max_tokens': -1, 'temperature': 0.0, 'top_p': 1.0, 'stop': None, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'other': {}, 'seed': None, 'response_format': None, 'tools': [], 'linked_tools': [], 'commit_message': None, 'name': 'medqa-answer', 'version_id': 'prv_T5dyAsua401BiJAnaN9O4', 'type': 'prompt', 'environments': [{'id': 'env_NakuPkQXr8w4dYkTAAynO', 'created_at': '2024-04-29T08:01:19.415384', 'name': 'production', 'tag': 'default'}], 'created_at': '2024-08-23T15:36:36.316581', 'updated_at': '2024-08-23T15:45:43.484736', 'created_by': {'id': 'usr_iBV0KNZkHdwXZSIUSaMKG', 'email_address': 'pnhayes+@tcd.ie', 'full_name': 'Peter', 'platform_access': 'user'}, 'status': 'uncommitted', 'last_used_at': '2024-08-23T15:36:36.316581', 'version_logs_count': 51, 'total_logs_count': 51, 'inputs': [{'name': 'question'}, {'name': 'option_A'}, {'name': 'option_B'}, {'name': 'option_C'}, {'name': 'option_D'}, {'name': 'option_E'}, {'name': 'retrieved_data'}, {'name': 'messages'}], 'team_id': 'tm_b79syTwUvFjr0T1tmT8wq', 'dashboard_configuration': None, 'evaluators': None, 'evaluator_aggregates': None}, batch_id=None, orchestrated=False)], evaluators=[EvaluationEvaluatorResponse(version=EvaluatorResponse(path='evals_demo/levenshtein', id='ev_dKPEnvBLVaEFh32xOAsg2', directory_id='dir_WFiBDkGinaPccJigBSHQ1', commit_message='New version from levenshtein.py', spec=CodeEvaluatorRequest(arguments_type='target_required', return_type='number', evaluator_type='python', code='def levenshtein_distance_optimized(s1, s2, max_distance=1000):\\n    \"\"\"\\n    Calculate the Levenshtein distance between two strings with optimizations and a maximum distance cap.\\n\\n    This function trims common prefixes and suffixes from the input strings, uses a single-row table\\n    to reduce space complexity, and stops the computation early if the Levenshtein distance is\\n    guaranteed to exceed a maximum distance cap.\\n\\n    Args:\\n        s1 (str): The first string.\\n        s2 (str): The second string.\\n        max_distance (int, optional): The maximum Levenshtein distance. Defaults to 1000.\\n\\n    Returns:\\n        int: The Levenshtein distance between the two strings, or max_distance if the distance\\n        exceeds max_distance.\\n    \"\"\"\\n    # Trim common prefixes\\n    while s1 and s2 and s1[0] == s2[0]:\\n        s1 = s1[1:]\\n        s2 = s2[1:]\\n\\n    # Trim common suffixes\\n    while s1 and s2 and s1[-1] == s2[-1]:\\n        s1 = s1[:-1]\\n        s2 = s2[:-1]\\n\\n    len_s1 = len(s1)\\n    len_s2 = len(s2)\\n\\n    # If the length difference between the strings exceeds max_distance, stop the computation\\n    if abs(len_s1 - len_s2) > max_distance:\\n        return max_distance\\n\\n    # If one of the strings is empty, the distance is the length of the other string\\n    if len_s1 == 0:\\n        return min(len_s2, max_distance)\\n    if len_s2 == 0:\\n        return min(len_s1, max_distance)\\n\\n    # Create a single-row table with len(s2) + 1 columns\\n    distance = list(range(len_s2 + 1))\\n\\n    # Fill up the table\\n    for i in range(1, len_s1 + 1):\\n        # Store the value of the previous cell in the previous row\\n        prev_row_cell = i - 1\\n        # The value at the first column is the row number\\n        distance[0] = i\\n\\n        # Initialize the minimum distance in the current row to max_distance\\n        min_distance = max_distance\\n\\n        for j in range(1, len_s2 + 1):\\n            # Store the value of the current cell before it is updated\\n            current_cell = distance[j]\\n\\n            # If the current characters of the two strings are the same, the cost is 0, otherwise 1\\n            substitution_cost = 0 if s1[i - 1] == s2[j - 1] else 1\\n\\n            # The value at the current cell is the minimum of the values at the previous cell in the\\n            # current row, the current cell in the previous row, and the previous cell in the previous row,\\n            # plus the cost\\n            distance[j] = min(\\n                distance[j - 1] + 1,  # deletion\\n                distance[j] + 1,  # insertion\\n                prev_row_cell + substitution_cost,\\n            )  # substitution\\n\\n            # Update the minimum distance in the current row\\n            min_distance = min(min_distance, distance[j])\\n\\n            # Update the value of the previous cell in the previous row\\n            prev_row_cell = current_cell\\n\\n        # If the minimum distance in the current row exceeds max_distance, stop the computation\\n        if min_distance >= max_distance:\\n            return max_distance\\n\\n    # The Levenshtein distance between the two strings is the value at the last cell in the table\\n    return min(distance[-1], max_distance)\\n\\n\\ndef extract_answer(log):\\n    \"\"\"Extracts answer from generation.\\n\\n    Handles a generation that if separated by \"---\" with the answer being the first part.\\n    Also handles a generation that starts with \"```\\\\n\" and removes it.\\n    \"\"\"\\n    generation = log[\"output\"]\\n\\n    answer = generation.split(\"---\")[0].strip()\\n    if answer.startswith(\"```\\\\n\"):\\n        answer = answer[4:].strip()\\n\\n    return answer\\n\\n\\ndef compare_log_and_target(log, testcase):\\n    target = testcase[\"target\"][\"output\"]\\n    return levenshtein_distance_optimized(target, extract_answer(log))\\n'), name='levenshtein', version_id='evv_M5xe0twPRlBLNDjWq9xmg', type='evaluator', environments=[EnvironmentResponse(id='env_NakuPkQXr8w4dYkTAAynO', created_at=datetime.datetime(2024, 4, 29, 8, 1, 19, 415384), name='production', tag='default')], created_at=datetime.datetime(2024, 8, 23, 15, 38, 18, 281287), updated_at=datetime.datetime(2024, 8, 23, 15, 38, 18, 281287), created_by={'id': 'usr_iBV0KNZkHdwXZSIUSaMKG', 'email_address': 'pnhayes+@tcd.ie', 'full_name': 'Peter', 'platform_access': 'user'}, status='committed', last_used_at=datetime.datetime(2024, 8, 23, 15, 38, 18, 281287), version_logs_count=0, total_logs_count=0, inputs=[], evaluators=None, evaluator_aggregates=None, default_evaluator=False, team_id='tm_b79syTwUvFjr0T1tmT8wq', dashboard_configuration=None), orchestrated=True), EvaluationEvaluatorResponse(version=EvaluatorResponse(path='evals_demo/exact_match', id='ev_jhXvIub7cNVux37tq50mD', directory_id='dir_WFiBDkGinaPccJigBSHQ1', commit_message='New version from exact_match.py', spec=CodeEvaluatorRequest(arguments_type='target_required', return_type='boolean', evaluator_type='python', code='def extract_answer(log):\\n    \"\"\"Extracts answer from generation.\\n\\n    Handles a generation that if separated by \"---\" with the answer being the first part.\\n    Also handles a generation that starts with \"```\\\\n\" and removes it.\\n    \"\"\"\\n    generation = log[\"output\"]\\n\\n    answer = generation.split(\"---\")[0].strip()\\n    if answer.startswith(\"```\\\\n\"):\\n        answer = answer[4:].strip()\\n\\n    return answer\\n\\n\\ndef exact_match(log, testcase):\\n    target = testcase[\"target\"][\"output\"]\\n    return target == extract_answer(log)\\n'), name='exact_match', version_id='evv_Yy0fDdiNKQAlY9jW7JGs1', type='evaluator', environments=[EnvironmentResponse(id='env_NakuPkQXr8w4dYkTAAynO', created_at=datetime.datetime(2024, 4, 29, 8, 1, 19, 415384), name='production', tag='default')], created_at=datetime.datetime(2024, 8, 23, 15, 38, 17, 957507), updated_at=datetime.datetime(2024, 8, 23, 15, 38, 17, 957507), created_by={'id': 'usr_iBV0KNZkHdwXZSIUSaMKG', 'email_address': 'pnhayes+@tcd.ie', 'full_name': 'Peter', 'platform_access': 'user'}, status='committed', last_used_at=datetime.datetime(2024, 8, 23, 15, 38, 17, 957507), version_logs_count=0, total_logs_count=0, inputs=[], evaluators=None, evaluator_aggregates=None, default_evaluator=False, team_id='tm_b79syTwUvFjr0T1tmT8wq', dashboard_configuration=None), orchestrated=True)], status='completed', created_at=datetime.datetime(2024, 8, 23, 15, 42, 7, 698298), created_by={'id': 'usr_iBV0KNZkHdwXZSIUSaMKG', 'email_address': 'pnhayes+@tcd.ie', 'full_name': 'Peter', 'platform_access': 'user'}, updated_at=datetime.datetime(2024, 8, 23, 15, 42, 8, 628408), url='https://stg.humanloop.com/project/pr_W0F7hDRYvtvRZywP98dL5/evaluations/evr_z0uhjU1QQp1IcjolUAhBI/stats')\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46b28006061d7b8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
