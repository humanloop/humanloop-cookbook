{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b81e0e5be92583",
   "metadata": {},
   "source": [
    "# Humanloop RAG Evaluation Walkthrough\n",
    "The goal of this notebook is to demonstrate how to take an existing RAG pipeline and integrate Humanloop in order to:\n",
    "1. Setup logging for both your [Prompt](https://humanloop.com/docs/v5/concepts/prompts) and retriever [Tool](https://humanloop.com/docs/v5/concepts/prompts) so that you can easily track the versions of these components.\n",
    "2. Create a [Dataset](https://humanloop.com/docs/v5/concepts/prompts) and run Evaluations to benchmark the performance of your RAG pipeline.\n",
    "3. Configure [Evaluators](https://humanloop.com/docs/v5/concepts/evaluators) for monitoring your RAG pipeline in production.\n",
    "\n",
    "\n",
    "## What is Humanloop?\n",
    "Humanloop is an interactive development environment designed to streamline the entire lifecycle of LLM app development. It serves as a central hub where AI, Product, and Engineering teams can collaborate on Prompt management, Evaluation and Monitoring workflows. \n",
    "\n",
    "\n",
    "## What is RAG?\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "- **Retrieval** - Getting the relevant information from a larger data source for a given a query.\n",
    "- **Augmented** - Using the retrieved information as input to an LLM.\n",
    "- **Generation** - Generating an output from the model given the input.\n",
    "\n",
    "In practise, it remains an effective way to exploit LLMs for things like question answering, summarization, and more, where the data source is too large to fit in the context window of the LLM, or where providing the full data source for each query is not cost-effective.\n",
    "\n",
    "\n",
    "## What are the major challenges with RAG?\n",
    "Implementing RAG and other similar flows complicates the process of [Prompt Engineering](https://humanloop.com/blog/prompt-engineering-101) because you expand the design space of your application. There are lots of choices you need to make around the retrieval and Prompt components that can significantly impact the performance of your overall application. For example,\n",
    "- How do you select the data source?\n",
    "- How should it be chunked up and indexed?\n",
    "- What embedding and retrieval model should you use?\n",
    "- How should you combine the retrieved information with the query?\n",
    "- What should your system Prompt be? \n",
    "- Which model should you use?\n",
    "- What should your system message be?\n",
    "etc...\n",
    "\n",
    "The process of versioning, evaluating and monitoring your pipeline therefore needs to consider both the retrieval and generation components. This is where Humanloop can help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a5f234d7bac09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53965cd45425bab5",
   "metadata": {},
   "source": [
    "# Example RAG Pipeline\n",
    "\n",
    "We first need a reference RAG implementation. Our use case will be Q&A over a corpus of medical documents.\n",
    "\n",
    "- **Dataset**: We'll use a version of the [MedQA dataset](https://huggingface.co/datasets/bigbio/med_qa) from Hugging Face. This is a multiple choice question answering problem based on the United States Medical License Exams (USMLE), with reference text books that contain the required information to answer the questions.\n",
    "- **Retriever**: We're going to use [Chroma](https://docs.trychroma.com/getting-started) as a simple local vector DB with their default embedding model ``. You can replace this with your favorite retrieval system.\n",
    "- **Prompt**: **The Prompt will be managed in code**, populated with the users question and the context retrieved from the Retriever and sent to [OpenAI](https://platform.openai.com/docs/api-reference/introduction) to generate the answer.\n",
    "\n",
    "### NB: where to store your Prompts?\n",
    "\n",
    "Generally speaking, when the engineering/applied AI teams are mainly responsible for managing the details of the Prompt, then the pattern of storing or constructing the Prompt in code works well. This is the pattern we follow in this tutorial. \n",
    "\n",
    "However, if the Product/Domain Expert teams are more involved in Prompt engineering and management, then the Prompt can instead be managed on Humanloop and retrieved or called by your code - this workflow lies outside the scope of this tutorial and we cover it separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da007e8494c60446",
   "metadata": {},
   "source": [
    "## Complete Pre-requisites\n",
    "\n",
    "### Install packages\n",
    "We use poetry to manage dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3e3f74731b50ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T11:20:18.947373Z",
     "start_time": "2024-08-21T11:20:14.779075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001B[36mchromadb\u001B[39m\n",
      "  - \u001B[36mopenai\u001B[39m\n",
      "  - \u001B[36mpandas\u001B[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Using version \u001B[39;1m^17.0.0\u001B[39;22m for \u001B[36mpyarrow\u001B[39m\n",
      "\n",
      "\u001B[34mUpdating dependencies\u001B[39m\n",
      "\u001B[2K\u001B[34mResolving dependencies...\u001B[39m \u001B[39;2m(1.1s)\u001B[39;22m\n",
      "\n",
      "\u001B[39;1mPackage operations\u001B[39;22m: \u001B[34m1\u001B[39m install, \u001B[34m0\u001B[39m updates, \u001B[34m0\u001B[39m removals\n",
      "\n",
      "  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mPending...\u001B[39m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m0%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m10%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m20%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m30%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m40%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m50%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m60%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m70%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m80%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m90%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mDownloading...\u001B[39m \u001B[39;1m100%\u001B[39;22m\n",
      "\u001B[1A\u001B[0J  \u001B[34;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[39;1m17.0.0\u001B[39;22m\u001B[39m)\u001B[39m: \u001B[34mInstalling...\u001B[39m\n",
      "\u001B[1A\u001B[0J  \u001B[32;1m-\u001B[39;22m \u001B[39mInstalling \u001B[39m\u001B[36mpyarrow\u001B[39m\u001B[39m (\u001B[39m\u001B[32m17.0.0\u001B[39m\u001B[39m)\u001B[39m\n",
      "\n",
      "\u001B[34mWriting lock file\u001B[39m\n"
     ]
    }
   ],
   "source": "!poetry install"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialise the SDKs\n",
    "\n",
    "You will need to set your OpenAI API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [OpenAI account](https://platform.openai.com/api-keys).\n"
   ],
   "id": "80f74709ea0177a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:34:28.376205Z",
     "start_time": "2024-08-23T10:34:28.362540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up dependencies\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from chromadb import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load .env file that contains API keys\n",
    "load_dotenv()\n",
    "\n",
    "# init clients\n",
    "chroma = chromadb.Client()\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n"
   ],
   "id": "47ac94aa",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set up the Vector DB\n",
    "This involves loading the data from the MedQA dataset and embedding the data within a collection in Chroma. This will take a couple of minutes to complete."
   ],
   "id": "f74ffee245036bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:33:19.186321Z",
     "start_time": "2024-08-23T10:33:17.116514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init collection into which we will add documents\n",
    "collection = chroma.get_or_create_collection(name=\"MedQA\")\n",
    "\n",
    "# load knowledge base\n",
    "knowledge_base = pd.read_parquet(\"../../assets/sources/textbooks.parquet\")\n",
    "knowledge_base = knowledge_base.sample(2, random_state=42)\n",
    "\n",
    "\n",
    "# Add to Chroma - will by default use local vector DB and model all-MiniLM-L6-v2\n",
    "collection.add(\n",
    "    documents=knowledge_base[\"contents\"].to_list(),\n",
    "    ids=knowledge_base[\"id\"].to_list(),\n",
    ")"
   ],
   "id": "15c5158d1d159535",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the Prompt\n",
    "We define a simple prompt template that has variables for the question, answer options and retrieved data.\n",
    "\n",
    "It is generally good practise to define the Prompt details that impact the behaviour of the model in one place separate to your application logic."
   ],
   "id": "cd43484af0f2f10c"
  },
  {
   "cell_type": "code",
   "id": "187af8c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:33:21.870523Z",
     "start_time": "2024-08-23T10:33:21.867055Z"
    }
   },
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0\n",
    "template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Answer the following question factually.\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Options:\n",
    "- {{option_A}}\n",
    "- {{option_B}}\n",
    "- {{option_C}}\n",
    "- {{option_D}}\n",
    "- {{option_E}}\n",
    "\n",
    "---\n",
    "\n",
    "Here is some retrieved information that might be helpful.\n",
    "Retrieved data:\n",
    "{{retrieved_data}}\n",
    "\n",
    "---\n",
    "\n",
    "Give you answer in 3 sections using the following format. Do not include the quotes or the brackets. Do include the \"---\" separators.\n",
    "```\n",
    "<chosen option verbatim>\n",
    "---\n",
    "<clear explanation of why the option is correct and why the other options are incorrect. keep it ELI5.>\n",
    "---\n",
    "<quote relevant information snippets from the retrieved data verbatim. every line here should be directly copied from the retrieved data>\n",
    "```\n",
    "\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "def populate_template(template: list, inputs: dict[str, str]) -> list:\n",
    "    \"\"\"Populate a template with input variables.\"\"\"\n",
    "    messages = []\n",
    "    for i, template_message in enumerate(template):\n",
    "        content = template_message[\"content\"]\n",
    "        for key, value in inputs.items():\n",
    "            content = content.replace(\"{{\" + key + \"}}\", value)\n",
    "        message = {**template_message, \"content\": content}\n",
    "        messages.append(message)\n",
    "    return messages\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the RAG Pipeline\n",
    "\n",
    "Now we provide the reference RAG pipeline using Chroma and OpenAI that takes a question and returns an answer. This is ultimately what we will evaluate.\n"
   ],
   "id": "e86410ee6d885c4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:34:34.969898Z",
     "start_time": "2024-08-23T10:34:34.967003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieval_tool(question: str) -> str:\n",
    "    \"\"\"Retrieve most relevant document from the vector db (Chroma) for the question.\"\"\"\n",
    "    response = collection.query(query_texts=[question], n_results=1)\n",
    "    retrieved_doc = response[\"documents\"][0][0]\n",
    "    return retrieved_doc\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict[str, str])-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    messages = populate_template(template, inputs)\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "    return answer"
   ],
   "id": "53c95ad9790ade59",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "da57f2c4b4533a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:34:37.242452Z",
     "start_time": "2024-08-23T10:34:36.886055Z"
    }
   },
   "source": [
    "# Test the pipeline\n",
    "\n",
    "print(\n",
    "    ask_question(\n",
    "        {\n",
    "            \"question\": \"A 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\",\n",
    "            'option_A': '0%', 'option_B': '25%', 'option_C': '50%', 'option_D': '75%', 'option_E': '100%'\n",
    "        }\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: <INSERT KEY>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test the pipeline\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mask_question\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mA 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moption_A\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m0\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moption_B\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m25\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moption_C\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m50\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moption_D\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m75\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moption_E\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m100\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m )\n",
      "Cell \u001B[0;32mIn[12], line 14\u001B[0m, in \u001B[0;36mask_question\u001B[0;34m(inputs)\u001B[0m\n\u001B[1;32m     12\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrieved_data\u001B[39m\u001B[38;5;124m\"\u001B[39m: retrieved_data}\n\u001B[1;32m     13\u001B[0m messages \u001B[38;5;241m=\u001B[39m populate_template(template, inputs)\n\u001B[0;32m---> 14\u001B[0m chat_completion \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m answer \u001B[38;5;241m=\u001B[39m chat_completion\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m answer\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    272\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/openai/resources/chat/completions.py:668\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    665\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    666\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m    667\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m--> 668\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/openai/_base_client.py:1260\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1247\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1248\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1255\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1256\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1257\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1258\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1259\u001B[0m     )\n\u001B[0;32m-> 1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/openai/_base_client.py:937\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    930\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    935\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    936\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 937\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    938\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    939\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    943\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/humanloop-cookbook-qc8tmMfn-py3.11/lib/python3.11/site-packages/openai/_base_client.py:1041\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1038\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1040\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1044\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1045\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1049\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[1;32m   1050\u001B[0m )\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <INSERT KEY>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "f9fc3cffe1b5cb99",
   "metadata": {},
   "source": [
    "# Humanloop Integration\n",
    "\n",
    "We now integrate Humanloop into the RAG pipeline to first enable logging and then to trigger evaluations against a dataset.\n",
    "\n",
    "\n",
    "### Initialise the SDK\n",
    "You will need to set your Humanloop API key in the  `.env` file in the root of the repo. You can retrieve your API key from your [Humanloop organization](https://app.humanloop.com/account/api-keys).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Init the Humanloop SDK\n",
    "from humanloop import Humanloop\n",
    "\n",
    "load_dotenv()\n",
    "humanloop = Humanloop(api_key=os.getenv(\"HUMANLOOP_KEY\"))"
   ],
   "id": "6c2102bcad49c932"
  },
  {
   "cell_type": "markdown",
   "id": "f6d598fd",
   "metadata": {},
   "source": [
    "## Integrate Logging\n",
    "\n",
    "Below, we add a `humanloop.tools.log(...)` call after the retrieval step to log the retrieved documents to Humanloop,\n",
    "and a `humanloop.prompts.log(...)` call after the chat completion generation.\n",
    "We also pass in a `session_id` to link these two Logs together.\n",
    "\n",
    "On running this updated code, Humanloop will now begin to track the versions of your Tool and Prompt and their inputs, outputs and associated metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985a6fa7b2f095da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the ask_question function to include logging\n",
    "\n",
    "def ask_question(inputs: dict[str, str])-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "\n",
    "    session_id = uuid.uuid4().hex\n",
    "    humanloop.tools.log(\n",
    "        path=\"evals_demo/medqa-retrieval\",\n",
    "        tool={\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval_tool\",\n",
    "                \"description\": \"Retrieval tool for MedQA.\",\n",
    "            },\n",
    "            \"source_code\": inspect.getsource(retrieval_tool),\n",
    "        },\n",
    "        output=retrieved_data,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    messages = populate_template(template, inputs)\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "\n",
    "    humanloop.prompts.log(\n",
    "        path=\"evals_demo/medqa-answer\",\n",
    "        prompt={\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"template\": template,\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        output=chat_completion.choices[0].message.content,\n",
    "        output_message=chat_completion.choices[0].message,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the pipeline\n",
    "\n",
    "print(\n",
    "    ask_question(\n",
    "        {\n",
    "            \"question\": \"A 34-year-old male suffers from inherited hemophilia A. He and his wife have three unaffected daughters. What is the probability that the second daughter is a carrier of the disease?\",\n",
    "            'option_A': '0%', 'option_B': '25%', 'option_C': '50%', 'option_D': '75%', 'option_E': '100%'\n",
    "        }\n",
    "    )\n",
    ")"
   ],
   "id": "acfe1f5d648b980"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Check your Humanloop workspace\n",
    "\n",
    "After running this pipeline, you will now see your Prompt and Tool logs in your Humanloop workspace:\n",
    "\n",
    "If you make changes to your Prompt in code and re-run the pipeline, you will a new version of the Prompt created in Humanloop:\n",
    "\n",
    "<INSERT PICTURE>\n"
   ],
   "id": "c5cf453bc0e7a6b7"
  },
  {
   "cell_type": "markdown",
   "id": "595e5f31dc65ba72",
   "metadata": {},
   "source": [
    "# Triggering Evaluations\n",
    "\n",
    "We will now extend our implementation to allow us to run Evaluations on Humanloop against a specific test dataset.\n",
    "\n",
    "This involves the following steps:\n",
    "1. Extend our logging to include info needed by Evaluations.\n",
    "2. Create a Dataset that we can manage and re-use on Humanloop as the source of truth.\n",
    "3. Create some Evaluators that we can manage and re-use on Humanloop that can provide judgements on the performance of our Pipeline.\n",
    "4. Trigger an Evaluation and view the results.\n",
    "\n",
    "Now as you tweak your pipeline, this will allow you to easily track and compare the performance of different versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272f102",
   "metadata": {},
   "source": [
    "## Extend logging\n",
    "\n",
    "Add `source_datapoint_id` to the `humanloop.prompt.log(...)` and `humanloop.tool.log(...)` calls.\n",
    "We do this below by adding the optional `datapoint_id` argument to `ask_question(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d208b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import uuid\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict[str, str], datapoint_id: str | None = None, evaluation_id: str| None = None)-> str:\n",
    "    \"\"\"Ask a question and get an answer using a simple RAG pipeline\"\"\"\n",
    "    retrieved_data = retrieval_tool(inputs[\"question\"])\n",
    "\n",
    "    session_id = uuid.uuid4().hex\n",
    "    humanloop.tools.log(\n",
    "        path=\"evals_demo/medqa-retrieval\",\n",
    "        tool={\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval_tool\",\n",
    "                \"description\": \"Retrieval tool for MedQA.\",\n",
    "            },\n",
    "            \"source_code\": inspect.getsource(retrieval_tool),\n",
    "        },\n",
    "        output=retrieved_data,\n",
    "        session_id=session_id,\n",
    "        source_datapoint_id=datapoint_id,\n",
    "        evaluation_id=evaluation_id,\n",
    "    )\n",
    "\n",
    "    inputs = {**inputs, \"retrieved_data\": retrieved_data}\n",
    "    messages = populate_template(template, inputs)\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "\n",
    "    humanloop.prompts.log(\n",
    "        path=\"evals_demo/medqa-answer\",\n",
    "        prompt={\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"template\": template,\n",
    "        },\n",
    "        inputs=inputs,\n",
    "        output=chat_completion.choices[0].message.content,\n",
    "        output_message=chat_completion.choices[0].message,\n",
    "        session_id=session_id,\n",
    "        source_datapoint_id=datapoint_id,\n",
    "        evaluation_id=evaluation_id,\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78969c2f",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "Here we will create a Dataset on Humanloop using the MedQA test dataset. Alternatively you can create a data from Logs on Humanloop, or upload via the UI - see our [guide](https://humanloop.com/docs/v5/evaluation/guides/create-dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87c70e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_humanloop():\n",
    "    df = pd.read_json(\"../../assets/datapoints.jsonl\", lines=True)\n",
    "\n",
    "    datapoints = [row.to_dict() for _i, row in df.iterrows()]\n",
    "    return humanloop.datasets.upsert(\n",
    "        path=\"evals_demo/medqa-test\",\n",
    "        datapoints=datapoints,\n",
    "        commit_message=f\"Added {len(datapoints)} datapoints from MedQA test dataset.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deae78c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnprocessableEntityError",
     "evalue": "status_code: 422, body: detail={'loc': ['commit_message'], 'msg': 'Error creating Version', 'description': \"Version 'dsv_No7Ofzh5RrFPWBbfujGfy' has already been committed.\", 'type': 'invalid_request_error'}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnprocessableEntityError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mupload_dataset_to_humanloop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[56], line 5\u001B[0m, in \u001B[0;36mupload_dataset_to_humanloop\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_json(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../assets/datapoints.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m datapoints \u001B[38;5;241m=\u001B[39m [row\u001B[38;5;241m.\u001B[39mto_dict() \u001B[38;5;28;01mfor\u001B[39;00m _i, row \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39miterrows()]\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mhumanloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevals_demo/medqa-test\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatapoints\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatapoints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcommit_message\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAdded \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdatapoints\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m datapoints from MedQA test dataset.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/code/humanloop/humanloop-cookbook/venv/lib/python3.11/site-packages/humanloop/datasets/client.py:243\u001B[0m, in \u001B[0;36mDatasetsClient.upsert\u001B[0;34m(self, datapoints, version_id, environment, path, id, action, commit_message, request_options)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m typing\u001B[38;5;241m.\u001B[39mcast(DatasetResponse, construct_type(type_\u001B[38;5;241m=\u001B[39mDatasetResponse, object_\u001B[38;5;241m=\u001B[39m_response\u001B[38;5;241m.\u001B[39mjson()))  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m422\u001B[39m:\n\u001B[0;32m--> 243\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m UnprocessableEntityError(\n\u001B[1;32m    244\u001B[0m             typing\u001B[38;5;241m.\u001B[39mcast(HttpValidationError, construct_type(type_\u001B[38;5;241m=\u001B[39mHttpValidationError, object_\u001B[38;5;241m=\u001B[39m_response\u001B[38;5;241m.\u001B[39mjson()))  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    245\u001B[0m         )\n\u001B[1;32m    246\u001B[0m     _response_json \u001B[38;5;241m=\u001B[39m _response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError:\n",
      "\u001B[0;31mUnprocessableEntityError\u001B[0m: status_code: 422, body: detail={'loc': ['commit_message'], 'msg': 'Error creating Version', 'description': \"Version 'dsv_No7Ofzh5RrFPWBbfujGfy' has already been committed.\", 'type': 'invalid_request_error'}"
     ]
    }
   ],
   "source": "dataset = upload_dataset_to_humanloop()"
  },
  {
   "cell_type": "markdown",
   "id": "3ffb5c76",
   "metadata": {},
   "source": [
    "### Set up Evaluators\n",
    "\n",
    "Here we will upload some Evaluators defined in code in `assets/evaluators/` so that Humanloop can manage running these for Evaluations.\n",
    "\n",
    "Alternatively you can define AI, Code and Human based Evaluators via the UI - see the relevant `How-to guides` on [Evaluations](https://humanloop.com/docs/v5/evaluation/overview) for creating Evaluators of different kinds.\n",
    "\n",
    "Further you can choose to not host the Evaluator on Humanloop and instead use your own runtime and instead post the results as part of the Evaluation. This can be useful for more complex workflows that require custom dependencies or resources, but lies outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dcb9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_evaluators():\n",
    "    for evaluator_name, return_type in [\n",
    "        (\"exact_match\", \"boolean\"),\n",
    "        (\"levenshtein\", \"number\"),\n",
    "    ]:\n",
    "        with open(f\"../../assets/evaluators/{evaluator_name}.py\", \"r\") as f:\n",
    "            code = f.read()\n",
    "        humanloop.evaluators.upsert(\n",
    "            path=f\"evals_demo/{evaluator_name}\",\n",
    "            spec={\n",
    "                \"evaluator_type\": \"python\",\n",
    "                \"arguments_type\": \"target_required\",\n",
    "                \"return_type\": return_type,\n",
    "                \"code\": code,\n",
    "            },\n",
    "            commit_message=f\"New version from {evaluator_name}.py\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8138e15d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnprocessableEntityError",
     "evalue": "status_code: 422, body: detail={'loc': ['commit_message'], 'msg': 'Error creating Version', 'description': \"Version 'evv_w85ulWbWFqElPWxXe3lEu' has already been committed.\", 'type': 'invalid_request_error'}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnprocessableEntityError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mupload_evaluators\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[58], line 8\u001B[0m, in \u001B[0;36mupload_evaluators\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../assets/evaluators/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevaluator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      7\u001B[0m     code \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m----> 8\u001B[0m \u001B[43mhumanloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluators\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevals_demo/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mevaluator_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevaluator_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpython\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43marguments_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtarget_required\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreturn_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcommit_message\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNew version from \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mevaluator_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.py\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/code/humanloop/humanloop-cookbook/venv/lib/python3.11/site-packages/humanloop/evaluators/client.py:208\u001B[0m, in \u001B[0;36mEvaluatorsClient.upsert\u001B[0;34m(self, spec, path, id, commit_message, request_options)\u001B[0m\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m typing\u001B[38;5;241m.\u001B[39mcast(EvaluatorResponse, construct_type(type_\u001B[38;5;241m=\u001B[39mEvaluatorResponse, object_\u001B[38;5;241m=\u001B[39m_response\u001B[38;5;241m.\u001B[39mjson()))  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m422\u001B[39m:\n\u001B[0;32m--> 208\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m UnprocessableEntityError(\n\u001B[1;32m    209\u001B[0m             typing\u001B[38;5;241m.\u001B[39mcast(HttpValidationError, construct_type(type_\u001B[38;5;241m=\u001B[39mHttpValidationError, object_\u001B[38;5;241m=\u001B[39m_response\u001B[38;5;241m.\u001B[39mjson()))  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    210\u001B[0m         )\n\u001B[1;32m    211\u001B[0m     _response_json \u001B[38;5;241m=\u001B[39m _response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError:\n",
      "\u001B[0;31mUnprocessableEntityError\u001B[0m: status_code: 422, body: detail={'loc': ['commit_message'], 'msg': 'Error creating Version', 'description': \"Version 'evv_w85ulWbWFqElPWxXe3lEu' has already been committed.\", 'type': 'invalid_request_error'}"
     ]
    }
   ],
   "source": [
    "upload_evaluators()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48f645",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we will define the function to run the Evaluation Humanloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12405c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "DATASET = \"evals_demo/medqa-test\"\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"Runs an Evaluation.\"\"\"\n",
    "    \n",
    "    # Create the Evaluation specifying the Dataset and Evaluators to use\n",
    "    evaluation = humanloop.evaluations.create(\n",
    "        # NB: you can also use the `id` to reference Datasets and Evaluators \n",
    "        dataset={\"path\": DATASET},\n",
    "        evaluators=[\n",
    "            {\"path\": \"evals_demo/exact_match\"},\n",
    "            {\"path\": \"evals_demo/levenshtein\"},\n",
    "        ],\n",
    "    )\n",
    "    print(f\"Evaluation created: {evaluation.id}\")\n",
    "    \n",
    "    # Run you pipeline over the Dataset\n",
    "    retrieved_dataset = humanloop.datasets.get(\n",
    "        path=DATASET,\n",
    "        include_datapoints=True,\n",
    "    )\n",
    "    for datapoint in tqdm(retrieved_dataset.datapoints):\n",
    "        ask_question(\n",
    "            inputs=datapoint.inputs,\n",
    "            datapoint_id=datapoint.id,\n",
    "            evaluation_id=evaluation.id,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c951317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation created: evr_tDoZQgxw3ZCSV5EV7HTXy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 53/1273 [03:12<1:10:42,  3.48s/it]"
     ]
    }
   ],
   "source": [
    "run_evaluation()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Follow the URL to see the Evaluation report building on Humanloop",
   "id": "da1377e005e09b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
